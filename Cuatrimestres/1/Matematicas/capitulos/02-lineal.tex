%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Espacios vectoriales}

	\subsection{Definiciones}

		\begin{definicion}
			Un espacio vectorial $V$ consta de lo siguiente:

			\begin{enumerate}
				\item Un campo $\mathbb{F}$ de escalares
				\item Un conjunto no vacio de objetos denominados vectores
				\item Una operación denominada suma o adición que asocia a cada par de vectores $\alpha, \beta \in V$, un vector $\alpha + \beta \in V$ llamado suma de $\alpha$ y $\beta$, que cumple lo siguiente
				\begin{enumerate}
					\item $\alpha + \beta = \beta + \alpha \quad \forall \+ \alpha, \beta \in V$
					\item $\alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma \quad \forall \+ \alpha, \beta, \gamma \in V$
					\item $\exists \+ ! \vec{0} \ni \alpha + \vec{0} = \alpha$
					\item $\exists \+ ! - \alpha \in V \ni \alpha + (-\alpha) = \vec{0} \quad \forall \+ \alpha \in V$
				\end{enumerate}
				\item Una operación denominada multiplicación por escalares, que asocia a cada escalar $c \in \mathbb{F}$ un vector $c \alpha \in V$, de manera que:
				\begin{enumerate}
					\item $(c_1 c_2) \alpha = c_1 (c_2 \alpha) \quad \forall \+ c_1, c_2 \in \mathbb{F} \quad \forall \+ \alpha \in V$
					\item $c(\alpha + \beta) = c \alpha + c \beta \quad \forall \+ c \in \mathbb{F} \quad \forall \+ \alpha, \beta \in V$
					\item $(c_1 + c_2) \alpha = c_1 \alpha + c_2 \alpha \quad \forall \+ c_1, c_2 \in \mathbb{F} \quad \forall \+ \alpha \in V$
					\item $1 \cdot \alpha = \alpha \quad \forall \+ \alpha \in V$
				\end{enumerate}
			\end{enumerate}
		\end{definicion}

		\begin{ejercicio}
			Verificar que un campo $\mathbb{F}$ es un espacio vectorial sobre si mismo.
		\end{ejercicio}

		\begin{ejercicio}
			Verificar que $\mathbb{R}$ es un espacio vectorial sobre $\mathbb{R}$
		\end{ejercicio}

		\begin{ejercicio}
			Verificar que $\mathbb{Q}$ es un espacio vectorial sobre $\mathbb{Q}$
		\end{ejercicio}

		\begin{ejercicio}
			Verificar que $\mathbb{C}$ es un espacio vectorial sobre $\mathbb{C}$
		\end{ejercicio}

		\begin{ejercicio}
			Verificar que $\mathbb{R}$ es un espacio vectorial sobre $\mathbb{Q}$
		\end{ejercicio}

		\begin{ejercicio}
			Verificar que $\mathbb{C}$ es un espacio vectorial sobre $\mathbb{Q}$
		\end{ejercicio}

		\begin{ejercicio}
			Verificar que $\mathbb{Q}$ es un espacio vectorial sobre $\mathbb{R}$
		\end{ejercicio}

		\begin{ejercicio}
			Verificar que $\mathbb{R}$ es un espacio vectorial sobre $\mathbb{C}$
		\end{ejercicio}

		\begin{definicion}
			Sea $\mathbb{F}$ un campo y sea $n \in \mathbb{N}$.
			Definimos el espacio vectorial $\mathbb{F}^n$ como:

			\begin{equation}
				\mathbb{F}^n = \left\{ \begin{pmatrix} x_1 & x_2 & \dots & x_n \end{pmatrix} \mid x_i \in \mathbb{F} \right\}
			\end{equation}

			Dados los elementos $\alpha, \beta \in \mathbb{F}^n$ de la forma:

			\begin{eqnarray*}
				\alpha & = & \begin{pmatrix} x_1 & x_2 & \dots & x_n \end{pmatrix} \\
				\beta  & = & \begin{pmatrix} y_1 & y_2 & \dots & y_n \end{pmatrix} \\
			\end{eqnarray*}

			podemos definir la suma como:

			\begin{equation}
				\alpha + \beta = \begin{pmatrix} x_1 + y_1 & x_2 + y_2 & \dots & x_n + y_n \end{pmatrix}
			\end{equation}

			y la multiplicación por escalar como:

			\begin{equation}
				c \alpha = \begin{pmatrix} c x_1 & c x_2 & \dots & c x_n \end{pmatrix} \quad \forall \+ c \in \mathbb{F}
			\end{equation}
		\end{definicion}

		\begin{ejemplo}
			\faltante{Falta escribir ejemplo}
		\end{ejemplo}

		\begin{ejemplo}
			\faltante{Falta escribir ejemplo}
		\end{ejemplo}

		\begin{ejemplo}
			\faltante{Falta escribir ejemplo}
		\end{ejemplo}

		\begin{ejemplo}
			\faltante{Falta escribir ejemplo}
		\end{ejemplo}

		\begin{ejercicio}
			Verificar que $\mathbb{F}^n$ es un espacio vectorial sobre $\mathbb{F}$, en particular si $\mathbb{F} = \mathbb{R}$ y $n = 2$.
		\end{ejercicio}

		\begin{proposicion}
			Sea $V$ un espacio vectorial sobre $\mathbb{F}$, entonces se tiene que:

			\begin{equation}
				0 \cdot \alpha = \vec{0} \quad \forall \+ \alpha \in V
			\end{equation}
		\end{proposicion}

		\begin{definicion}
			Se dice que $\beta \in V$ es una combinación lineal de vectores $\alpha_1, \alpha_2, \dots, \alpha_n$ si existen $c_1, c_2, \dots, c_n \in \mathbb{F}$ tales que:

			\begin{equation}
				\beta = \sum_{i=1}^n c_i \alpha_i
			\end{equation}
		\end{definicion}

%-------------------------------------------------------------------------------

	\subsection{Subespacios vectoriales}

		\begin{definicion}
			Un subespacio de un espacio vectorial $V$, es un subconjunto $W$ de $V$ que con las operaciones heredadas de $V$, es un espacio vectorial sobre $\mathbb{F}$ 
		\end{definicion}

		\begin{observacion}
			Si $V$ es un espacio vectorial, $V$ y $\{\vec{0}\}$ se denominan los subespacios triviales de $V$.
		\end{observacion}

		\begin{proposicion}
			Un subconjunto no vacio $W$ de $V$, es un subespacio vectorial si y solo si $W$ es cerrado con respecto a las operaciones de $V$.
		\end{proposicion}

		\begin{definicion}
			Sean $\alpha_1, \alpha_2, \dots, \alpha_k \in V$, definimos:

			\begin{equation}
				\mathcal{L}(\alpha_1, \alpha_2, \dots, \alpha_k) = \left\{ v \mid v \text{ es combinación lineal de } \alpha_1, \alpha_2, \dots, \alpha_k \right\}
			\end{equation}

			es decir, es un subespacio vectorial de $V$ y se llama subespacio generado por $\alpha_i$ con $1 \leq i \leq k$, o bien se dice que $\alpha_1, \alpha_2, \dots, \alpha_k$ generan a $\mathcal{L}(\alpha_1, \alpha_2, \dots, \alpha_k)$.

			En general, si $A \ne 0$ y si $A \subset V$, entonces

			\begin{equation}
				\mathcal{L}(A) = \left\{ v \mid v \text{ es combinacion lineal de los elementos de } A \right\}
			\end{equation}
		\end{definicion}

		\begin{proposicion}
			La intersección de cualquier colección de subespacios vectoriales de $V$ es un subespacio vectorial de $V$.
		\end{proposicion}

		\begin{proof}
			Sea $W a = \left\{ w a \mid a \in I \right\}$ y sea $W = \cap W a$.

			En primer lugar notamos que $W$ es no vacio, ya que $\vec{0} \in W a$ para todo $a \in I$.

			Despues tomamos dos elementos $\alpha, \beta \in W$, los cuales estan tambien en $W a$ para todo $a \in I$.
			Si los operamos entre si, sabemos que $\alpha + \beta \in W a$ para todo $a \in I$, por lo que sigue que tambien $\alpha + \beta \in W$.

			Por ultimo, si tomamos un elemento $\beta \in W$ y un elemento $r \in \mathbb{F}$, sabemos que $r \beta \in W a$ para todo $a \in I$, por lo que se sigue que $r \beta \in W$.

			Por lo que concluimos que $W$ es un subespacio vectorial.
		\end{proof}

		\begin{observacion}
			La union de subespacios vectoriales, no necesariamente es un subespacio vectorial.

			Por ejemplo, si en $\mathbb{R}^2$ definimos:

			\begin{eqnarray*}
				W_1 = \left\{ \begin{pmatrix} x_1 & 0 \end{pmatrix} \mid x_1 \in \mathbb{R} \right\} \subset \mathbb{R} \\
				W_2 = \left\{ \begin{pmatrix} 0 & x_2 \end{pmatrix} \mid x_2 \in \mathbb{R} \right\} \subset \mathbb{R}
			\end{eqnarray*}

			La union de estos subespacios vectoriales sería:

			\begin{equation*}
				W_1 \cup W_2 = \left\{ \begin{pmatrix} x_1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & x_2 \end{pmatrix} \mid x_1, x_2 \in \mathbb{R} \right\}
			\end{equation*}

			por lo que si tomamos un elemento de cada subespacio vectorial y los sumamos, obtendremos:

			\begin{equation*}
				\begin{pmatrix} x_1 & 0 \end{pmatrix} + \begin{pmatrix} 0 & x_2 \end{pmatrix} = \begin{pmatrix} x_1 & x_2 \end{pmatrix} \notin W_1 \cup W_2
			\end{equation*}
		\end{observacion}

		\begin{definicion}
			Sean $S, T$ subespacios vectoriales de $V$, definimos la suma de $S$ y $T$ como:

			\begin{equation}
				S + T = \left\{ s + t \mid s \in S, t \in T \right\}
			\end{equation}
		\end{definicion}

		\begin{proposicion}
			Si $S$ y $T$ son subespacios vectoriales de $V$, entonces $S + T$ es un subespacio de $V$.
		\end{proposicion}

		\begin{definicion}
			Si $S$ y $T$ son subespacios vectoriales de $V$, tales que $V = S + T$ y $S \cap T = \{0\}$, decimos que $V$ es la suma directa de $S$ y $T$, y la denotamos por:

			\begin{equation}
				V = S \oplus T 
			\end{equation}
		\end{definicion}

		\begin{proposicion}
			Si $V = S \oplus T$, entonces existen únicos $s$ y $t$ tales que:

			\begin{equation}
				\alpha = s + t \quad \forall \+ \alpha \in V
			\end{equation}
		\end{proposicion}

		\begin{proof}
			Sean $s, s' \in S$ y $t, t' \in T$ elementos para los que se cumple que un $\alpha \in V$, tenga la siguiente forma:

			\begin{equation*}
				\alpha = s + t = s' + t'
			\end{equation*}

			Si a los dos ultimos terminos de esta ecuación les restamos $s + t'$, tendremos:

			\begin{equation*}
				s + t - s - t' = s' + t' - s - t'
			\end{equation*}

			es decir:

			\begin{equation*}
				t - t' = s' - s
			\end{equation*}

			pero el lado izquierdo de la ecuación es algo que está en $T$ y el segundo termino es algo que está en $S$, es decir, estamos preguntando que elementos hay en común en $S$ y $T$.
			Como $V = S \oplus T$, sabemos que la intersección entre $S$ y $T$ es $\{0\}$, por lo que nos quedan las siguientes relaciones:

			\begin{eqnarray*}
				t - t' & = & 0 \\
				s' - s & = & 0
			\end{eqnarray*}

			lo cual implica que:

			\begin{eqnarray*}
				t & = & t' \\
				s' & = & s
			\end{eqnarray*}
		\end{proof}

		\begin{definicion}
			Sea $A$ una matriz $m \times n$ con entradas en $\mathbb{F}$, los vectores fila de $A$ son:

			\begin{eqnarray*}
				\alpha_1 & = & \begin{pmatrix} a_{11} & a_{12} & \dots & a_{1n} \end{pmatrix} \\
				\alpha_2 & = & \begin{pmatrix} a_{21} & a_{22} & \dots & a_{2n} \end{pmatrix} \\
				& \vdots & \\
				\alpha_m & = & \begin{pmatrix} a_{m1} & a_{m2} & \dots & a_{mn} \end{pmatrix}
			\end{eqnarray*}

			y los vectores columna son:

			\begin{eqnarray*}
				\beta_1 & = & \begin{pmatrix} a_{11} & a_{21} & \dots & a_{m1} \end{pmatrix}^T \\
				\beta_2 & = & \begin{pmatrix} a_{12} & a_{22} & \dots & a_{m2} \end{pmatrix}^T \\
				& \vdots & \\
				\beta_n & = & \begin{pmatrix} a_{1n} & a_{2n} & \dots & a_{mn} \end{pmatrix}^T
			\end{eqnarray*}

			es decir:

			\begin{equation*}
				A =
				\begin{pmatrix}
					a_{11} & a_{12} & \dots & a_{1n} \\
					a_{21} & a_{22} & \dots & a_{2n} \\
					\vdots & \vdots & & \vdots \\
					a_{m1} & a_{m2} & \dots & a_{mn}
				\end{pmatrix}
			\end{equation*}

			El subespacio de $\mathbb{F}^m$ generado por $\alpha_1, \alpha_2, \dots, \alpha_m$ se denomina espacio fila de $A$.
			El subespacio de $\mathbb{F}^n$ generado por $\beta_1, \beta_2, \dots, \beta_n$ se denomina espacio columna de $A$.
		\end{definicion}

		\begin{definicion}
			Si $A$ y $B$ son matrices equivalentes (por filas, mediante operaciones elementales), entonces el espacio fila de $A$ coincide con el espacio fila de $B$.
		\end{definicion}
 
%-------------------------------------------------------------------------------

	\subsection{Dependencia e independencia lineal}

		\begin{definicion}
			Los vectores $\alpha_1, \alpha_2, \dots, \alpha_k$ se dicen linealmente dependientes si existen escalares $a_1, a_2, \dots, a_k$ no todos cero, tales que:

			\begin{equation}
				\sum_{i=1}^k a_i \alpha_i = 0
			\end{equation}

			Los vectores $\alpha_1, \alpha_2, \dots, \alpha_k$ se dicen linealmente independientes si no son linealmente dependientes.
		\end{definicion}

		\begin{definicion}
			Un conjunto $A$ de vectores se dice linealmente independiente si cualquier subconjunto finito de $A$ es linealmente independiente.
		\end{definicion}

		\begin{definicion}
			Un conjunto $A$ de vectores se dice linealmente dependiente si existe un subconjunto finito de $A$ no vacio que sea linealmente dependiente.
		\end{definicion}

		\begin{observacion}
			Por convención, el conjunto $\varnothing$ es linealmente independiente.
		\end{observacion}

		\begin{ejemplo}
			Verificar si $\left\{ \begin{pmatrix} 1 & 2 \end{pmatrix}, \begin{pmatrix} 2 & 9 \end{pmatrix}, \begin{pmatrix} 0 & 3 \end{pmatrix} \right\}$ es linealmente dependiente o independiente.
			\faltante{Falta desarrollar ejemplo}
		\end{ejemplo}

		\begin{ejercicio}
			Verificar si $\left\{ \begin{pmatrix} 20 & 3 \end{pmatrix}, \begin{pmatrix} 0 & 0 \end{pmatrix} \right\}$ es linealmente dependiente o independiente en $\mathbb{R}^2$
		\end{ejercicio}

		\begin{ejercicio}
			Verificar si $\left\{ \begin{pmatrix} 0 & 0 & 0 \end{pmatrix} \right\}$ es linealmente dependiente o independiente en $\mathbb{R}^3$.
		\end{ejercicio}

		\begin{ejercicio}
			Verificar si $\left\{ e_1, e_2, e_3 \right\}$ es linealmente dependiente o independiente en $\mathbb{R}^4$.
		\end{ejercicio}

		\begin{proposicion}
			Un conjunto de vectores $\{\alpha_1, \alpha_2, \dots, \alpha_n\}$, con $\alpha_i \ne 0$ para $1 \leq i \leq n$ es linealmente dependiente si y solo si algún $\alpha_k$ es combinación lineal de los vectores que le proceden, con $1 \leq k \leq n$.
		\end{proposicion}

		\begin{proposicion}
			Si $\alpha_1, \alpha_2, \dots, \alpha_k$ con $\alpha_i \ne 0$ para $1 \leq i \leq k$ son vectores que generan a un espacio vectorial $V$, entonces existe un subconjunto linealmente independiente de estos generadores que tambien generan a $V$.
		\end{proposicion}

		\begin{proof}
			Para demostrar la proposición anterior, debemos tomar un subconjunto de los vectores originales y eliminar cualquier vector linealmente dependiente, para lograr que sea linealmente independiente, y que siga generando al espacio vectorial.

			Primero debemos de tomar el primer vector $\alpha_1$ \footnote{Realmente no importa el orden que tengan los vectores, ya que si un par es linealmente dependiente, tan solo debemos elimnar a uno de ellos.} y revisar si es linealmente dependiente con el siguiente vector $\alpha_2$, si es cierto esto, se procede a eliminar el vector $\alpha_2$ y se toma el siguiente vector, y asi hasta terminar con el conjunto.

			Al finalizar este proceso, tendremos un subconjunto $\{ \alpha_1, \alpha_{n_1}, \dots, \alpha_{n_j}\}$ que es linealmente independiente por construcción y que sigue generando al espacio vectorial.
		\end{proof}

		\begin{ejercicio}
			Sea $V$ un espacio vectorial, y sean $\alpha_1, \alpha_2, \alpha_3 \in V$ tales que:

			\begin{equation*}
				\alpha_1 + \alpha_2 + \alpha_3 = 0
			\end{equation*}

			Verificar que:

			\begin{equation*}
				\mathcal{L}\{ \alpha_1, \alpha_2 \} = \mathcal{L}\{ \alpha_2, \alpha_3 \}
			\end{equation*}
		\end{ejercicio}

		\begin{ejercicio}
			Sea el conjunto $A$ definido por:

			\begin{equation*}
				A = \left\{ x \in \mathbb{R} \mid x = a \sqrt{1} + b \sqrt{s} + c \sqrt{3}, a, b, c \in \mathbb{Q} \right\}
			\end{equation*}

			Verificar que $A$ es un espacio vectorial sobre $\mathbb{Q}$.

			Verificar que $\sqrt{1}, \sqrt{2}, \sqrt{3}$ son linealmente independientes y generan a $A$.
		\end{ejercicio}

		\begin{ejercicio}
			Verificar que:

			\begin{equation*}
				\left\{ 1, 1 + x, 1 + x + x^2, \dots, 1 + x + x^2 + \dots + x^n \right\}
			\end{equation*}

			es un conjunto con elementos linealmente independientes.
		\end{ejercicio}

		\begin{definicion}
			Sea $V$ un espacio vectorial de dimensión $n$, un conjunto de $n$ vectores forma una base para $V$ si es linealmente independiente y genera a $V$.
		\end{definicion}

		\begin{ejercicio}
			Encontrar una base para $\mathbb{R}^3$ que incluya a $\begin{pmatrix} 1 & 1 & 0 \end{pmatrix}$ y $\begin{pmatrix} 0 & 1 & 0 \end{pmatrix}$.
		\end{ejercicio}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Isomorfismos}

	\subsection{Definiciones}

		\begin{definicion}
			Sean $V$ y $V'$ espacios vectoriales sobre un campo $\mathbb{F}$, se dice que $V$ y $V'$ son isomorfos, si existe una función $\varphi \colon V \to V'$ inyectiva y suprayectiva, tal que:

			\begin{enumerate}
				\item $\varphi(\alpha + \beta) = \varphi(\alpha) + \varphi(\beta) \quad \forall \+ \alpha, \beta \in V$
				\item $\varphi(r \alpha) = r \varphi(\alpha) \quad \forall \+ r \in \mathbb{F} \quad \forall \+ \alpha \in V$
			\end{enumerate}
		\end{definicion}

		\begin{proposicion}
			El isomorfismo entre espacios vectoriales es una relación de equivalencia.
		\end{proposicion}

		\begin{teorema}
			Todo espacio vectorial $V$ sobre $\mathbb{F}$ de dimensión $n > 0 $ es isomorfo a $\mathbb{F}^n$, es decir:

			\begin{equation}
				V \cong \mathbb{F}^n
			\end{equation}
		\end{teorema}

		\begin{proof}
			Sea $\{ \alpha_1, \alpha_2, \dots, \alpha_n \}$ una base de $V$. Dado $\alpha \in V$, este tiene la forma:

			\begin{equation*}
				\alpha = a_1 \alpha_1 + a_2 \alpha_2 + \dots + a_n \alpha_n
			\end{equation*}

			Ahora, si definimos una función $\varphi$ tal que:

			\begin{equation*}
				\varphi(\alpha) =
				\begin{pmatrix}
					a_1 & a_2 & \dots & a_n
				\end{pmatrix}
			\end{equation*}

			es decir:

			\begin{eqnarray*}
				\varphi \colon V & \to & \mathbb{F}^n \\
				\alpha & \to & \begin{pmatrix} a_1 & a_2 & \dots & a_n \end{pmatrix}
			\end{eqnarray*}

			Por lo que tenemos que demostrar, es que esta función es isomorfismo.
			Para empezar, demostraremos que esta función es inyectiva.

			Para esto tomamos dos elementos $\alpha, \beta \in V$, tales que:

			\begin{eqnarray*}
				\alpha & = & a_1 \alpha_1 + a_2 \alpha_2 + \dots + a_n \alpha_n \\
				\beta & = & b_1 \alpha_1 + b_2 \alpha_2 + \dots + b_n \alpha_n
			\end{eqnarray*}

			Por lo que si asumimos que $\varphi(\alpha) = \varphi(\beta)$, es decir:

			\begin{equation}
				\begin{pmatrix}
					a_1 & a_2 & \dots & a_n
				\end{pmatrix} =
				\begin{pmatrix}
					b_1 & b_2 & \dots & b_n
				\end{pmatrix}
			\end{equation}

			tenemos que esto implica que:

			\begin{eqnarray*}
				a_1 & = & b_1 \\
				a_2 & = & b_2 \\
				& \vdots & \\
				a_n & = & b_n
			\end{eqnarray*}

			y por lo tanto $\alpha = \beta$, por lo que $\varphi$ es inyectiva.

			Por otro lado, sabemos que $\varphi$ es suprayectiva por construcción.

			Ahora tomamos dos elementos $\alpha, \beta \in V$ y los operamos, por lo que tendremos que:

			\begin{eqnarray*}
				\varphi(\alpha + \beta) & = & \varphi(a_1 \alpha_1 + a_2 \alpha_2 + \dots + a_n \alpha_n + b_1 \alpha_1 + b_2 \alpha_2 + \dots + b_n \alpha_n) \\
				& = & \varphi((a_1 + b_1) \alpha_1 + (a_2 + b_2) \alpha_2 + \dots + (a_n + b_n) \alpha_n) \\
				& = & \begin{pmatrix} a_1 + b_1 & a_2 + b_2 & \dots & a_n + b_n \end{pmatrix} \\
				& = & \begin{pmatrix} a_1 & a_2 & \dots & a_n \end{pmatrix} + \begin{pmatrix} b_1 & b_2 & \dots & b_n \end{pmatrix} \\
				& = & \varphi(\alpha) + \varphi(\beta)
			\end{eqnarray*}

			Por otro lado, si tomamos un elemento $r \in \mathbb{F}$ y un elemento $\alpha \in V$, tendremos que:

			\begin{eqnarray*}
				\varphi(r \alpha) & = & \varphi(r (a_1 \alpha_1 + a_2 \alpha_2 + \dots + a_n \alpha_n)) \\
				& = & \varphi(r a_1 \alpha_1 + r a_2 \alpha_2 + \dots + r a_n \alpha_n) \\
				& = & \begin{pmatrix} r a_1 & r a_2 & \dots & r a_n \end{pmatrix} \\
				& = & r \begin{pmatrix} a_1 & a_2 & \dots & a_n \end{pmatrix} = r \varphi(\alpha)
			\end{eqnarray*}

			Por lo que queda demostrado que $\varphi$ es un isomorfismo y por lo tanto:

			\begin{equation*}
				V \cong \mathbb{F}^n
			\end{equation*}
		\end{proof}

		\begin{ejercicio}
			Sea un espacio vectorial $V$ definido como:

			\begin{equation*}
				V = \left\{ p(x) = a_0 + a_1 x \mid \grado{p(x)} = 1 \right\}
			\end{equation*}

			Verfiicar que $V \cong \mathbb{R}^2$ dada la siguiente función:

			\begin{eqnarray*}
				\varphi \colon V & \to & \mathbb{R}^2 \\
				\alpha & \to & \begin{pmatrix} a_0 & a_1 \end{pmatrix}
			\end{eqnarray*}
		\end{ejercicio}

		\begin{proposicion}
			Si $S$ y $T$ son subespacios vectoriales de un espacio vectorial de dimensión finita, entonces tenemos que:

			\begin{equation}
				\dim{(S + T)} + \dim{(S \cap T)} = \dim{S} + \dim{T}
			\end{equation}
		\end{proposicion}

		\begin{proof}
			Sea $\{\alpha_1, \alpha_2, \dots, \alpha_k\}$ una base de $S \cap T$, podemos completar con $\{\beta_1, \beta_2, \dots, \beta_{r-k}\}$ a una base de $S$, por lo que la dimensión de $S$ sería:

			\begin{equation*}
				\dim{S} = k + (r - k) = r
			\end{equation*}

			Similarmente podemos completar a una base de $T$ con $\{\gamma_1, \gamma_2, \dots, \gamma_{m-k}\}$, por lo que su dimensión es:

			\begin{equation*}
				\dim{T} = k + (m - k) = m
			\end{equation*}

			Veamos que:

			\begin{equation*}
				\{ \underbrace{\overbrace{\alpha_1, \alpha_2, \dots, \alpha_k}^k, \overbrace{\beta_1, \beta_2, \dots, \beta_{r-k}}^{r-k}}_{r}, \overbrace{\gamma_1, \gamma_2, \dots, \gamma_{m-k}}^{m-k} \}
			\end{equation*}

			es una base de $S + T$, por lo que:

			\begin{eqnarray*}
				\dim{(S + T)} & = & r + m - k \\
				\dim{(S + T)} & = & \dim{S} + \dim{T} - \dim{(S \cap T)}
			\end{eqnarray*}

			Aqui aun falta verificar que esta base es linealmente independiente y que genera a $S + T$, pero puede verse inmediatamente el resultado final.
		\end{proof}

		\begin{ejercicio}
			Verificar que en $\mathbb{R}^3$ la suma de dos subespacios de dimensción igual a dos, tiene dimensiones no cero.
		\end{ejercicio}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Transformaciones lineales}

	\subsection{Definiciones}

		\begin{definicion}
			Sean $V, W$, espacios vectoriales sobre un campo $\mathbb{F}$, con una función $T \colon V \to W$.
			Se dice que $T$ es una transformación lineal de $V$ a $W$ si:

			\begin{enumerate}
				\item $T(\alpha + \beta) = T(\alpha) + T(\beta) \quad \forall \+ \alpha, \beta \in V$
				\item $T(c \alpha) = c T(\alpha) \quad \forall \+ c \in \mathbb{F} \quad \forall \+ \alpha \in V$
			\end{enumerate}
		\end{definicion}

		\begin{proposicion}
			Sea $T \colon V \to W$ una transformación lineal, sean $c_1, c_2, \dots, c_k \in \mathbb{F}$ y $\alpha_1, \alpha_2, \dots, \alpha_k \in V$, entonces:

			\begin{equation*}
				T(c_1 \alpha_1 + c_2 \alpha_2 + \dots + c_n \alpha_n) = c_1 T(\alpha_1) + c_2 T(\alpha_2) + \dots + c_n T(\alpha_n)
			\end{equation*}

			es decir:

			\begin{equation}
				T \left( \sum_{i=1}^n c_i \alpha_i \right) = \sum_{i=1}^n c_i T(\alpha_i) \quad n \in \mathbb{N}
			\end{equation}
		\end{proposicion}

		\begin{proposicion}
			Si $T \colon V \to W$ es una transformación lineal, entonces:

			\begin{equation}
				T(0_V) = 0_W
			\end{equation}
		\end{proposicion}

		\begin{proof}
			Si sumamos $0_W$ a $T(0_V)$, tendremos.

			\begin{equation*}
				0_W + T(0_V) = T(0_V) = T(0_V + 0_V) = T(0_V) + T(0_V)
			\end{equation*}

			y cancelando un $T(0_V)$ en ambos lados de esta ecuación, tenemos:

			\begin{equation*}
				0_W = T(0_V)
			\end{equation*}
		\end{proof}

		\begin{ejercicio}
			Dada la función:

			\begin{eqnarray*}
				0 \colon V & \to & V \\
				\alpha & \to & 0
			\end{eqnarray*}

			Verificar si es transformación lineal, si es inyectiva y si es suprayectiva.
		\end{ejercicio}

		\begin{ejercicio}
			Dada la función:

			\begin{eqnarray*}
				T \colon \mathbb{R} & \to & \mathbb{R} \\
				x & \to & x^2
			\end{eqnarray*}

			Verificar si es transformación lineal, si es inyectiva y si es suprayectiva.
		\end{ejercicio}

		\begin{ejercicio}
			Dada la función:

			\begin{eqnarray*}
				T \colon \mathbb{R}^3 & \to & \mathbb{R}^2 \\
				\begin{pmatrix} x_1 & x_2 & x_3 \end{pmatrix} & \to & \begin{pmatrix} x_1 & x_3 \end{pmatrix}
			\end{eqnarray*}

			Verificar si es transformación lineal, si es inyectiva y si es suprayectiva.
		\end{ejercicio}

		\begin{ejercicio}
			Dada la función:

			\begin{eqnarray*}
				T \colon \mathbb{R}^2 & \to & \mathbb{R}^3 \\
				\begin{pmatrix} x_1 & x_2 \end{pmatrix} & \to & \begin{pmatrix} 0 & x_1 & x_2 \end{pmatrix}
			\end{eqnarray*}

			Verificar si es transformación lineal, si es inyectiva y si es suprayectiva.
		\end{ejercicio}

		\begin{teorema}
			Sea $V, W$ espacios vectoriales sobre un campo $\mathbb{F}$, $\{\alpha_1, \alpha_2, \dots, \alpha_n\}$ una base de $V$ y $\{w_1, w_2, \dots, w_n\}$ una base de $W$, existe una única transformación lineal $T \colon V \to W$ tal que:

			\begin{equation}
				T(\alpha_i) = w_i \quad 1 \leq i \leq n
			\end{equation}
		\end{teorema}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Operadores lineales}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Funcionales lineales}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Espacio dual}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Teorema de Cayley - Hamilton}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Diagonalización}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Forma canónica de Jordan}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

\section{Vectores propios generalizados}