%-------------------------------------------------------------------------------
%	EMPIEZA CAPITULO
%-------------------------------------------------------------------------------

\chapter{Representación de estado}

    La siguiente funcion de transferencia es la Transformada de Laplace de la ecuacion diferencial ordinaria de orden $n$ que describe al sistema.

    \begin{equation}
        \frac{\hat{y}(s)}{\hat{u}(s)} = \frac{b_0 s^m + b_1 s^{m-1} + ... + b_{m-1} s + b_m}{s^n + a_1 s^{n-1} + ... + a_{n-1} s + a_n} = \frac{B(s)}{A(s)}, m \le n
    \end{equation}

    \begin{multline}
        \frac{d^n}{dt^n} y(t) + a_1 \frac{d^{n-1}}{dt^{n-1}} y(t) + \dots + a_{n-1} \frac{d}{dt} y(t) + a_n \frac{d}{dt} y(t) \\ = b_0 \frac{d^m}{dt^m} u(t) + b_1 \frac{d^{m-1}{dt^{m-1}}} u(t) + \dots + b_{m-1} \frac{d}{dt} u(t) + b_{m} u(t)
    \end{multline}

    Haciendo la siguiente asignacion de variables:

    \begin{eqnarray}
    x_1     & = & z \nonumber \\
    x_2     & = & \frac{d}{dt} x_1 = \frac{d}{dt} z \nonumber \\
    x_3     & = & \frac{d}{dt} x_2 = \frac{d^2}{dt^2} z \nonumber \\
    \vdots  & = & \vdots \nonumber \\
    x_{n-1} & = & \frac{d}{dt} x_{n-2} = \frac{d^{n-2}}{dt^{n-2}} z \nonumber \\
    x_n     & = & \frac{d}{dt} x_{n-1} = \frac{d^{n-1}}{d^{n-1}} z \nonumber
    \end{eqnarray}

    Donde:

    \begin{equation}
        \frac{d}{dt} x_n = -a_n x_1 - a_{n-1} x_2 - \dots - a_2 x_{n-1} - a_1 x_n + u(t)
    \end{equation}
    \begin{equation}
        y = b_m x_1 + b_{m-1} x_2 + \dots + b_1 x_{m-1} + b_0 x_m
    \end{equation}

    Por lo que se obtiene:

    \begin{equation*}
        \left( \frac{d^n}{dt^n} + a_1 \frac{d^{n-1}}{dt^{n-1}} + \dots + a_{n-1} \frac{d}{dt} + a_n \right) z(t) = u(t)
    \end{equation*}

    \begin{equation*}
        y(t) = \left( b_m + b_{m-1} \frac{d}{dt} + \dots + b_1 \frac{d^{m-1}}{dt^{m-1}} + b_0 \frac{d^m}{dt^m} \right) z(t)
    \end{equation*}

    es decir:

    \begin{equation}
        M \left( \frac{d}{dt} \right) z(t) = u(t)
    \end{equation}

    \begin{equation}
        y(t) = N \left( \frac{d}{dt} \right) z(t)
    \end{equation}

    Lo cual implica $M \left( \frac{d}{dt} \right) y(t) = N \left( \frac{d}{dt} \right) u(t)$. Donde:

    \begin{equation*}
        M \left( \frac{d}{dt} \right) = \left( \frac{d^n}{dt^n} + a_1 \frac{d^{n-1}}{dt^{n-1}} + \dots + a_{n-1} \frac{d}{dt} + a_n \right)
    \end{equation*}

    \begin{equation*}
        N \left( \frac{d}{dt} \right) = \left( b_m + b_{m-1} \frac{d}{dt} + \dots + b_1 \frac{d^{m-1}}{dt^{m-1}} + b_0 \frac{d^m}{dt^m} \right)
    \end{equation*}

    Esta es la misma ecuación diferencial con la que empezamos. Note que la escritura matricial de esta Ecuación Diferencial Ordinaria\footnote{Si bien la notación correcta es la que se utiliza justo ahora, al final del capitulo se dejará a un lado, para obviar el hecho de que son vectores y matrices, sin que por eso se entienda que ya no lo son.} es:

    \begin{equation}
        \frac{d}{dt} \vec{x} = A \vec{x}(t) + \vec{b} u(t)
    \end{equation}

    \begin{equation}
        \vec{y}(t) = \vec{c}^{T} \cdot \vec{x}(t)
    \end{equation}

    Donde:

    \begin{equation}
        \vec{x}(t) =
        \begin{pmatrix}
        x_1(t) \\
        x_2(t) \\
        \vdots \\
        x_n(t)
        \end{pmatrix}
    \end{equation}

    \begin{equation}
        A =
        \begin{pmatrix}
        0 & 1 & 0 & \dots & 0 & 0 \\
        0 & 0 & 1 & \dots & 0 & 0 \\
        0 & 0 & 0 & \dots & 0 & 0 \\
        \vdots & \vdots & \vdots &   & \vdots & \vdots \\
        0 & 0 & 0 & \dots & 0 & 1 \\
        -a_n & -a_{n-1} & -a_{n-2} & \dots & -a_2 & -a_1
        \end{pmatrix}
    \end{equation}

    \begin{equation}
        \vec{b} =
        \begin{pmatrix}
        0 \\
        0 \\
        \vdots \\
        1
        \end{pmatrix}
    \end{equation}

    \begin{equation}
        \vec{c} =
        \begin{pmatrix}
        b_m     \\
        b_{m-1} \\
        \vdots  \\
        b_1     \\
        b_0     \\
        \vdots  \\
        0       \\
        0
        \end{pmatrix}
    \end{equation}

    Por lo que esta representación puede ser vista de la siguiente manera en un diagrama de bloques.

    \begin{figure*}
    \centering
    \resizebox{\textwidth}{!}{
        \tikzstyle{block} = [draw, rectangle, minimum height=2.25em, minimum width=2.25em]

        \begin{tikzpicture}[auto, node distance=1.1cm, >=latex']
            \node [input, name=entrada] {};
            \node [sum, right of=entrada] (s1) {$+$};
            \node [block, right of=s1] (int1) {$\int$};
            \node [inner sep=0,minimum size=0,right of=int1] (xn) {};
            \node [block, right of=xn] (int2) {$\int$};
            \node [inner sep=0,minimum size=0,right of=int2] (xn1) {};
            \node [block, draw=none, right of=xn1] (vacio) {$\dots$};
            \node [inner sep=0,minimum size=0,right of=vacio] (x3) {};
            \node [block, right of=x3] (int3) {$\int$};
            \node [inner sep=0,minimum size=0,right of=int3] (x2) {};
            \node [block, right of=x2] (int4) {$\int$};
            \node [inner sep=0,minimum size=0,right of=int4] (x1) {};
            \node [block, right of=x1] (bm) {$b_m$};
            \node [sum, right of=bm] (s2) {$+$};
            \node [output, right of=s2] (salida) {};

            \node [block, below of=int1] (a1) {$-a_1$};
            \node [block, below of=a1] (a2) {$-a_2$};
            \node [block, draw=none, below of=a2] (avacio) {$\vdots$};
            \node [block, below of=avacio] (an1) {$-a_{n-1}$};
            \node [block, below of=an1] (an) {$-a_n$};

            \node [block, above of=bm] (bm1) {$b_{m-1}$};
            \node [block, draw=none, above of=bm1] (bvacio) {$\vdots$};
            \node [block, above of=bvacio] (b1) {$b_{1}$};
            \node [block, above of=b1] (b0) {$b_{0}$};

            \draw [->] (entrada) -- node[name=u] {$u(t)$} (s1);
            \draw [->] (s1)      -- (int1);
            \draw [->] (int1)    -- node[name=xn] {$x_n$} (int2);
            \draw [->] (int2)    -- node[name=xn1] {$x_{n-1}$} (vacio);
            \draw [->] (vacio)   -- node[name=x3] {$x_3$} (int3);
            \draw [->] (int3)    -- node[name=x2] {$x_2$} (int4);
            \draw [->] (int4)    -- node[name=x1] {$x_1$} (bm);
            \draw [->] (bm)      -- (s2);
            \draw [->] (s2)      -- node[name=y] {$y(t)$} (salida);

            \draw [->] (xn)  |- (a1);
            \draw [->] (xn1) |- (a2);
            \draw [->] (x2)  |- (an1);
            \draw [->] (x1)  |- (an);

            \draw [->] (a1)  -| (s1.324);
            \draw [->] (a2)  -| (s1.288);
            \draw [->] (an1) -| (s1.252);
            \draw [->] (an)  -| (s1.216);

            \draw [->] (xn)  |- (b0);
            \draw [->] (xn1) |- (b1);
            \draw [->] (x2)  |- (bm1);

            \draw [->] (b0)  -| (s2.45);
            \draw [->] (b1)  -| (s2.90);
            \draw [->] (bm1) -| (s2.135);
        \end{tikzpicture}}
    \caption{\label{dia:represtado1}Diagrama de bloques de una representación en espacio de estados.}
    \end{figure*}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

    \newpage
    \section{Solución temporal de la ecuación de estado}

    \begin{enumerate}

    \item Para el caso en que $A$ es un escalar y la solución es homogénea se considera la siguiente Ecuación Diferencial Ordinaria:

        \begin{equation}
            \frac{d}{dt} x(t) = a x(t) \mid x(0) = x_0
        \end{equation}

        Suponga una solución de la forma:

        \begin{equation}
            x(t) = \alpha_0 + \alpha_1 t + \alpha_2 t^2 + \dots + \alpha_k t^k + \dots
        \end{equation}

        Entonces se tiene:

        \begin{multline}
            \alpha_1 + 2 \alpha_2 t + 3 \alpha_3 t^2 + \dots + k \alpha_k t^{k-1} + \dots \\ = a \alpha_0 + a \alpha_1 t + a \alpha_2 t^2 + \dots + a \alpha_k t^k + \dots \nonumber
        \end{multline}

        Por lo que las $\alpha_{i}$ deben satisfacer:

        \begin{equation}
            \begin{array}{c c c c c}
            \alpha_1 & = & a \alpha_0     & = & \frac{1}{1!} a^1 \alpha_0 \\
            \alpha_2 & = & a \alpha_1     & = & \frac{1}{2!} a^2 \alpha_0 \\
            \alpha_3 & = & a \alpha_2     & = & \frac{1}{3!} a^3 \alpha_0 \\
            \vdots   & = & \vdots         & = & \vdots                    \\
            \alpha_k & = & a \alpha_{k-1} & = & \frac{1}{k!} a^k \alpha_0 \\
            \end{array} \quad ; \quad \alpha_0 = x_0
        \end{equation}

        Esto es:

        \begin{equation}
            x(t) = \left( \sum\limits_{i = 0}^{\infty} \frac{1}{i!} (a t)^i \right) x_0 \nonumber
        \end{equation}

        \begin{equation}
            x(t) = e^{at} x_0
        \end{equation}

        Notese que:

        \begin{equation*}
            \frac{d}{dt} x(t) = \left( \sum\limits_{i=0}^{\infty} \frac{1}{i!} \frac{d}{dt} (at)^i \right) x_{0} = \left( \sum\limits_{i=1}^{\infty} \frac{1}{(i-1)!} (at)^{i-1} \right) a x_{0} = \left( \sum\limits_{j=0}^{\infty} \frac{1}{j!} (at)^j \right) a x_{0}
        \end{equation*}

        \begin{equation*}
            \frac{d}{dt} x(t) = a e^{at} x_{0} = a x(t) \quad x(0) = x_{0}
        \end{equation*}

    \item Para el caso en que $A$ es una matriz y la solución es homogénea se considera la siguiente Ecuación Diferencial Ordinaria:

        \begin{equation}
            \frac{d}{dt} \vec{x}(t) = a \vec{x}(t) \mid \vec{x}(0) = \vec{x}_0
        \end{equation}

        De la misma manera que en el caso escalar, se supone una solución de la forma:

        \begin{equation}
            \vec{x}(t) = \vec{\alpha}_0 + \vec{\alpha}_1 t + \vec{\alpha}_2 t^2 + \dots + \vec{\alpha}_k t^k + \dots
        \end{equation}

        Entonces se tiene:

        \begin{multline}
            \vec{\alpha}_1 + 2 \vec{\alpha}_2 t + 3 \vec{\alpha}_3 t^2 + \dots + k \vec{\alpha}_k t^{k-1} + \dots \\ = A \vec{\alpha}_0 + A \vec{\alpha}_1 t + A \vec{\alpha}_2 t^2 + \dots + A \vec{\alpha}_k t^k + \dots \nonumber
        \end{multline}

        Por lo que las $\vec{\alpha}_i$ deben satisfacer:

        \begin{equation}
            \begin{array}{c c c c c}
            \vec{\alpha}_1 & = & A \vec{\alpha}_0     & = & \frac{1}{1!} A^1 \vec{\alpha}_0 \\
            \vec{\alpha}_2 & = & A \vec{\alpha}_1     & = & \frac{1}{2!} A^2 \vec{\alpha}_0 \\
            \vec{\alpha}_3 & = & A \vec{\alpha}_2     & = & \frac{1}{3!} A^3 \vec{\alpha}_0 \\
            \vdots   & = & \vdots         & = & \vdots                    \\
            \vec{\alpha}_k & = & A \vec{\alpha}_{k-1} & = & \frac{1}{k!} A^k \vec{\alpha}_0 \\
            \end{array} \quad ; \quad \vec{\alpha}_0 = \vec{x}_0
        \end{equation}

        Esto es:

        \begin{equation}
            \vec{x}(t) = \left( \sum\limits_{i = 0}^{\infty} \frac{1}{i!} (A t)^i \right) \vec{x}_0 \nonumber
        \end{equation}

        En análisis real, se demuestra que esta serie es absolutamente convergente y se define como:

        \begin{equation}
            \exp{(At)} = \sum\limits_{i = 0}^{\infty} \frac{1}{i!} (A t)^i
        \end{equation}

        Notese que:

        \begin{equation*}
            \frac{d}{dt} \exp{(At)} = \frac{d}{dt} \sum\limits_{i=0}^{\infty} \frac{1}{i!} (At)^i = \left( \sum\limits_{i=1}^{\infty} \frac{1}{(i-1)!} (At)^{i-1} \right) A = A \sum\limits_{j=0}^{\infty} \frac{1}{j!} (At)^j = A \exp{(At)}
        \end{equation*}

        Por lo que:

        \begin{equation*}
            \vec{x}(t) = \exp{(At)} \vec{x}_0
        \end{equation*}

        \begin{equation*}
            \frac{d}{dt} \vec{x}(t) = A \exp{(At)} \vec{x}_0 = A \vec{x}(t) \quad \vec{x}(0) = \vec{x}_0
        \end{equation*}

    \item Para el caso en que $A$ es escalar y la solución es forzada:

        \begin{equation}
            \frac{d}{dt} x(t) = a x(t) + b u(t) \mid x(0) = 0
        \end{equation}

        La solución a esta ecuación es:

        \begin{equation}
            x(t) = \int_0^t e^{a(t-\tau)} b u(\tau) \, d \tau
        \end{equation}

        \begin{equation*}
            \frac{d}{dt} x(t) = e^{a(t-t)} b u(t) + \int_0^t \frac{d}{dt} e^{a(t-\tau)} b u(\tau) \, d \tau = b u(t) + a \int_0^t e^{a(t-\tau)} b u(\tau) \, d \tau
        \end{equation*}

        \begin{equation}
            \frac{d}{dt} x(t) = b u(t) + a x(t)
        \end{equation}

        Por lo que la solución general (con $x(0) = x_0$):

        \begin{equation}
            x(t) = e^{at} x_0 + \int_0^t e^{a(t-\tau)} b u(\tau) \, d \tau
        \end{equation}

    \item Para el caso en que $A$ es una matriz y la solución es forzada:

        \begin{equation}
            \frac{d}{dt} \vec{x}(t) = A \vec{x}(t) + \vec{b} u(t) \mid \vec{x}(0) = 0
        \end{equation}

        La solución de esta ecuación es:

        \begin{equation}
            \vec{x}(t) = \int_0^t \exp{(A(t-\tau))} \vec{b} u(\tau) \, d \tau
        \end{equation}

        En efecto, derivando tenemos:

        \begin{equation*}
            \frac{d}{dt} \vec{x}(t) = \exp{(A(t-t))} \vec{b} u(t) + \int_0^t \frac{d}{dt} \exp{(A(t-\tau))} \vec{b} u(\tau) \, d \tau = \vec{b} u(t) + A \int_0^t \exp{(A(t-\tau))} \vec{b} u(\tau) \, d \tau
        \end{equation*}

        \begin{equation}
            \frac{d}{dt} \vec{x}(t) = \vec{b} u(t) + a \vec{x}(t)
        \end{equation}

        Por lo que la solución general (con $x(0) = x_0$):

        \begin{equation}
            \vec{x}(t) = \exp{(At)} \vec{x}_0 + \int_0^t \exp{(A(t-\tau))} \vec{b} u(\tau) \, d \tau
        \end{equation}

    \end{enumerate}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

    \newpage
    \section{Función (Matriz) de transferencia de la ecuación de estado}

        \begin{enumerate}

        \item
        Para el caso escalar, se tiene que la transformada de Laplace con coeficientes independientes nulos es:

        \begin{align}
        s x(s)       & = a x(s) + b u(s) \nonumber\\
        (s - a) x(s) & = b u(s) \nonumber\\
        x(s)         & = (s - a)^{-1} b u(s) \nonumber\\
        x(s)         & = \frac{b}{s - a} u(s) \nonumber
        \end{align}

        Por lo que:

        \begin{equation}
        e^{at} = \mathcal{L}^{-1} \left\{ (s - a)^{-1} \right\}
        \end{equation}

        \item
        Para el caso matricial, tenemos que la transformada de Laplace con coeficientes independientes nulos es:

        \begin{align}
        s \vec{x}(s)         & = A \vec{x}(s) + \vec{b} u(s) \nonumber \\
        (s I - A) \vec{x}(s) & = \vec{b} u(s) \nonumber \\
        x(s)                 & = (s I - A)^{-1} \vec{b} u(s) \nonumber
        \end{align}

        Por lo que:

        \begin{equation}
        \exp{(At)} = \mathcal{L}^{-1} \left\{ (s I - A)^{-1} \right\}
        \end{equation}

        \end{enumerate}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

    \newpage
    \section{Función de transferencia de la representación de estado}

        Sea la siguiente Ecuación Diferencial Ordinaria:

        \begin{equation}
            M \left( \frac{d}{dt} \right) y(t) = N \left( \frac{d}{dt} \right) u(t) \nonumber
        \end{equation}

        donde:

        \begin{equation*}
            M \left( \frac{d}{dt} \right) = \frac{d^n}{dt^n} + a_1 \frac{d^{n-1}}{dt^{n-1}} + \dots + a_{n-1} \frac{d}{dt} + a_n
        \end{equation*}

        \begin{equation*}
            N \left( \frac{d}{dt} \right) = b_m + b_{m-1} \frac{d}{dt} + \dots + b_1 \frac{d^{m-1}}{dt^{m-1}} + b_0 \frac{d^m}{dt^m}
        \end{equation*}

        La función de transferencia con coeficientes independientes nulos de las ecuaciones es:

        \begin{equation}
            F(s) = \frac{N(s)}{M(s)} = \frac{\frac{d^n}{dt^n} + a_1 \frac{d^{n-1}}{dt^{n-1}} + \dots + a_{n-1} \frac{d}{dt} + a_n}{b_m + b_{m-1} \frac{d}{dt} + \dots + b_1 \frac{d^{m-1}}{dt^{m-1}} + b_0 \frac{d^m}{dt^m}}
        \end{equation}

        \begin{description}
            \item [Ceros.] Las raíces del polinomio $N(s)$.
            \item [Polos.] Las raíces del polinomio $M(s)$.
        \end{description}

        Sea la siguiente representación de estado de la Ecuación Diferencial Ordinaria:

        \begin{eqnarray}
        \frac{d}{dt} x & = & A x + b u \nonumber \\
        y & = & c^T x + d u \nonumber
        \end{eqnarray}

        La función de transferencia con coeficientes independientes nulos en esta representación es:

        \begin{eqnarray}
        s x(s) & = & A x(s) + b u(s) \nonumber \\
        y(s) & = & c^T x(s) + d u(s) \nonumber
        \end{eqnarray}

        \begin{equation}
            \vdots \nonumber
        \end{equation}

        \begin{eqnarray}
        (s I - A) x(s) & = & b u(s) \nonumber \\
        y(s) & = & c^T x(s) + d u(s) \nonumber
        \end{eqnarray}

        \begin{equation}
            \vdots \nonumber
        \end{equation}

        \begin{eqnarray}
        x(s) & = & (s I - A)^{-1} b u(s) \nonumber \\
        y(s) & = & c^T x(s) + d u(s) \nonumber
        \end{eqnarray}

        \begin{equation}
            \vdots \nonumber
        \end{equation}

        \begin{equation}
            y(s) = c^T[(s I - A)^{-1} b u(s)] + d u(s) = [c^T(s I - A)^{-1} b + d] u(s) \nonumber
        \end{equation}

        \begin{equation}
            \vdots \nonumber
        \end{equation}

        \begin{equation}
            F(s) = c^T(s I - A)^{-1} b + d
        \end{equation}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

    \newpage
    \section{Matriz sistema}

        \begin{equation}
            \Sigma(s) =
            \begin{pmatrix}
            sI - A & b \\
            -c^T & d
            \end{pmatrix}
        \end{equation}

        Note que:

        \begin{multline}
            \begin{pmatrix}
            (sI - A)^{-1} & 0 \\
            0 & I
            \end{pmatrix}
            \begin{pmatrix}
            sI - A & b \\
            -c^T & d
            \end{pmatrix}
            \begin{pmatrix}
            I & -(sI - A)b \\
            0 & I
            \end{pmatrix}
            \\
            =
            \begin{pmatrix}
            I & (sI - A)^{-1} b \\
            -c^T & d
            \end{pmatrix}
            \begin{pmatrix}
            I & -(sI - A)b \\
            0 & I
            \end{pmatrix}
            \\
            =
            \begin{pmatrix}
            I & 0 \\
            -c^T & (c^T(sI - A)^{-1} b + d)
            \end{pmatrix}
            \nonumber
        \end{multline}

        Por lo que:

        \begin{equation*}
            \det{\left( (sI - A)^{-1} \right)} \cdot \det{\left( \Sigma(s) \right)} \cdot I = c^T(sI - A)^{-1} b + d
        \end{equation*}

        \begin{equation}
            F(s) = \frac{\det{\left( \Sigma(s) \right)}}{\det{\left( sI - A \right)}}
        \end{equation}

        Por lo que los polos coinciden con los valores propios de $A$ y los ceros son los números complejos que hacen perder rango a la matriz sistema.

        \begin{equation}
            \text{Polos: } F(s) = \left\{ s \in \mathbb{C} \mid \det{\left( sI - A \right)} = 0 \right\}
        \end{equation}

        \begin{equation}
            \text{Ceros: } F(s) = \left\{ s \in \mathbb{C} \mid \det{\left( \Sigma(s) \right)} = 0 \right\}
        \end{equation}

%-------------------------------------------------------------------------------
%	EMPIEZA SECCION
%-------------------------------------------------------------------------------

    \newpage
    \section{Propiedades de la Matriz A}
        \begin{enumerate}[i)]
            \item Definción de la matriz exponencial.
            \begin{equation}
                \exp{(A t)} = \sum\limits_{i=0}^{\infty} \frac{1}{i!} (A t)^i
            \end{equation}

            \item Derivada de la matriz exponencial.
            \begin{equation}
                \frac{d}{dt} \exp{(A t)} = A \exp{(A t)} = (\exp{(A t)}) A
            \end{equation}

            \item Linealidad del operador matriz exponencial bajo escalar.
            \begin{multline}
                \exp{(At)} \exp{(A \tau)} = \left( \sum\limits_{i=0}^{\infty} \frac{1}{i!} (A t)^i \right) \left( \sum\limits_{j=0}^{\infty} \frac{1}{j!} (A \tau)^j \right) \\
                = \sum\limits_{i=0}^{\infty} \sum\limits_{j=0}^{\infty} A^{i+j} \frac{t^i \tau^j}{i! j!} = \sum\limits_{k=0}^{\infty} A^k \sum\limits_{i=0}^{k} \frac{t^i \tau^{k-i}}{i! (k-i)!} \\
                = \sum\limits_{k=0}^{\infty} A^k \frac{(t + \tau)^k}{k!} = \exp{(A(t + \tau))}
            \end{multline}

            \item Linealidad del operador matriz exponencial bajo matriz.
            \begin{equation}
                \exp{((A + B) t)} = \exp{(A t)} \exp{(B t)} \iff A B = B A
            \end{equation}

            \item Cambio de base.

            Sean dos matrices similares $A$ y $\bar{A}$, esto es, dos matrices relacionadas por un cambio de base, $T$ matriz invertible, esto es $\bar{A} = T^{-1} A T$.

            \begin{enumerate}[a)]
                \item Las matrices exponenciales asociadas a las matrices $A$ y $\bar{A}$ también son similares. En efecto:

                \begin{multline}
                    T^{-1} \exp{(At)} T = T^{-1} \left( \sum\limits_{i=0}^{\infty} \frac{1}{i!} (A t)^i \right) T = \sum\limits_{i=0}^{\infty} \frac{1}{i!} T^{-1} A^i T t^i \\
                    = \sum\limits_{i=0}^{\infty} \frac{1}{i!} (T^{-1} A T)^i t^i = \exp{\bar{A} t} \nonumber
                \end{multline}

                \item Los valores propios son invariantes bajo cambio de base. En efecto:

                \begin{multline}
                    \det{(sI - \bar{A})} = \det{(sI - T^{-1} A T)} = \det{(s T^{-1} T - T^{-1} A T)} \\
                    = \det{(T^{-1} (sI - A) T)} = \det{(T^{-1})} \det{(sI - A)} \det{(T)} \\
                    = \frac{1}{\det{(T)}} \det{(sI - A)} \det{(T)} = \det{(sI - A)} \nonumber
                \end{multline}

                \item Las raíces de la matriz sistema son invariantes bajo cambio de base. En efecto, sea el sistema representado por:

                \begin{eqnarray}
                \frac{d}{dt} x & = & A x + b u \nonumber \\
                y & = & c^T x + d u \nonumber
                \end{eqnarray}

                Sea el cambio de variable $x = T \bar{x}$, $T$ invertible. Entonces:

                \begin{eqnarray}
                T \frac{d}{dt} \bar{x} & = & A T \bar{x} + b u \nonumber \\
                y & = & c^T T \bar{x} + d u \nonumber
                \end{eqnarray}

                \begin{equation}
                    \vdots \nonumber
                \end{equation}

                \begin{eqnarray}
                \frac{d}{dt} \bar{x} & = & T^{-1} A T \bar{x} + T^{-1} b u \nonumber \\
                y & = & c^T T \bar{x} + d u \nonumber
                \end{eqnarray}

                \begin{equation}
                    \vdots \nonumber
                \end{equation}

                \begin{eqnarray}
                \frac{d}{dt} \bar{x} & = & \bar{A} \bar{x} + \bar{b} u \nonumber \\
                y & = & \bar{c}^T \bar{x} + d u
                \end{eqnarray}

                donde $\bar{A} = T^{-1} A T$, $\bar{b} = T^{-1} b$, $\bar{c}^T = c^T T$. La matriz sistema se puede escribir de la siguiente manera:

                \begin{equation}
                    \Sigma =
                    \begin{pmatrix}
                    sI - A & b \\
                    -c^T & d
                    \end{pmatrix}
                    \implies
                    \bar{\Sigma} =
                    \begin{pmatrix}
                    sI - \bar{A} & \bar{b} \\
                    -\bar{c}^T & d
                    \end{pmatrix}
                \end{equation}

                Notese que:

                \begin{equation}
                    \bar{\Sigma} =
                    \begin{pmatrix}
                    sI - \bar{A} & \bar{b} \\
                    -\bar{c}^T & d
                    \end{pmatrix} =
                    \begin{pmatrix}
                    T^{-1} & 0 \\
                    0 & 1
                    \end{pmatrix}
                    \begin{pmatrix}
                    sI - A & b \\
                    -c^T & d
                    \end{pmatrix}
                    \begin{pmatrix}
                    T & 0 \\
                    0 & 1
                    \end{pmatrix} \nonumber
                \end{equation}

                Por lo que:

                \begin{equation}
                    \det{\bar{\Sigma}} = \det{T^{-1}} \det{\Sigma} \det{T} = \det{\Sigma} \nonumber
                \end{equation}

            \end{enumerate}

            \item Forma de Jordan

            Dada una matriz $A$, existe una matriz de cambio de base $T$, tal que:

            \begin{equation}
                T^{-1} A T = J = D + N
            \end{equation}

            donde $D$ es una matriz diagonal (conteniendo los valores propios) y $N$ es una matriz nilpotente ($\exists \gamma \in \mathbb{N} \mid N^{\gamma} = 0$) de la forma:

            \begin{equation}
                D =
                \begin{pmatrix}
                \lambda_1 & 0 & \dots & 0 \\
                0 & \lambda_2 & \dots & 0 \\
                \vdots & \vdots & & \vdots \\
                0 & 0 & \dots & \lambda_n
                \end{pmatrix} \nonumber
            \end{equation}

            \begin{equation}
                N =
                \begin{pmatrix}
                0 & 1 & 0 & \dots & 0 \\
                0 & 0 & 1 & \dots & 0 \\
                \vdots & \vdots & \vdots & & \vdots \\
                0 & 0 & 0 & \dots & 1 \\
                0 & 0 & 0 & \dots & 0
                \end{pmatrix} \nonumber
            \end{equation}

            Note que $D N = N D$, por lo que:

            \begin{equation}
                \exp{((D + N) t)} = \exp{(D t)} \exp{(N t)}
            \end{equation}

            donde:

            \begin{equation}
                \exp{(D t)} =
                \begin{pmatrix}
                e^{\lambda_1 t} & 0 & \dots & 0 \\
                0 & e^{\lambda_2 t} & \dots & 0 \\
                \vdots & \vdots & & \vdots \\
                0 & 0 & \dots & e^{\lambda_n t}
                \end{pmatrix} \nonumber
            \end{equation}

            \begin{equation}
                \exp{(N t)} = \sum\limits_{i=0}^{\infty} \frac{1}{i!} (N t)^i = \sum\limits_{i=0}^{\gamma - 1} \frac{1}{i!} (N t)^i \nonumber
            \end{equation}

            \item Teorema de Cayley-Hamilton

            Toda transformación lineal $A$ satisface su polinomio característico.

            \begin{equation}
                \Pi(s) = \det{(sI - A)} = s^n + \pi_1 s^{n-1} + \dots + \pi_{n-1} s + \pi_n
            \end{equation}

            \begin{equation}
                \Pi(A) = A^n + \pi_1 A^{n-1} + \dots + \pi_{n-1} A + \pi_n I = 0
            \end{equation}

            Una implicación directa es que la $n$-esima potencia de una transformación lineal $A$, es una combinación lineal de sus potencias predecesoras.

            \begin{equation}
                A^n = - \pi_n I - \pi_{n-1} A - \dots - \pi_1 A^{n-1} \nonumber
            \end{equation}

            A su vez, esto implica:

            \begin{equation}
                \exp{(A t)} = \sum\limits_{i=0}^{\infty} \frac{1}{i!} A^i t^i = \sum\limits_{i=0}^{n-1} \varphi(t) A^n
            \end{equation}

            donde:

            \begin{equation}
                \varphi_i(t) = \sum\limits_{j=0}^{\infty} \varphi_{ij} t^j \nonumber
            \end{equation}

        \end{enumerate}
