\documentclass[12pt]{article}

\usepackage[spanish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{enumerate}

\numberwithin{equation}{subsection}

\title{Control Clásico}
\author{Roberto Cadena Vega}

\begin{document}
\maketitle

\newpage
\section{Criterio de Estabilidad de Routh-Hurwitz}

\begin{equation}
\frac{\hat{y}(s)}{\hat{R}(s)} = \frac{b_0 s^m + b_1 s^{m-1} + ... + b_{m-1} s + b_m}{a_0 s^n + a_1 s^{n-1} + ... + a_{n-1} s + a_n} = \frac{B(s)}{A(s)}
\end{equation}
\begin{equation}
\frac{\hat{y}(s)}{\hat{R}(s)} = \sum{\frac{k_{1,i}}{s + \alpha_i}} + \sum{\frac{k_{2,j} + k_{3,j} \cdot s}{(s + \beta_i)^2 + {\gamma_i}^2}} ; m \leq n
\end{equation}
El criterio de Routh-Hurwitz determina si existen raíces en el semiplano complejo derecho cerrado.

\subsection{Tabla de Routh}
La tabla de Routh es un método para obtener el numero de raíces con parte real positiva que se encontraran en el polinomio característico del sistema (Ecuación~\ref{eqn:ECS}) sin tener que calcular las raíces en cuestión.
Se puede dividir en cuatro pasos que se enumeran a continuación.

\begin{equation} \label{eqn:ECS}
A(s) = a_0 s^n + a_1 s^{n-1} + ... + a_{n-1} s + a_n = 0
\end{equation}

\begin{enumerate}
\item
Hipótesis

Si \begin{math}a_0 = 0 \Rightarrow\end{math} el polinomio es de orden menor a \begin{math}n\end{math}.

Si \begin{math}a_n = 0 \Rightarrow \exists\end{math} una raíz que es \begin{math}0 \Rightarrow A(s) = (\bar{n_0} s^{\bar{n}} + \bar{a_n} s^{\bar{n-1}} + ...) s^k \end{math}.

\item
Si existen coeficientes nulos o de diferente (cambio de) signo, entonces existen raíces con parte real positiva.\newpage
\item
Construir la tabla de Routh (Ver Cuadro~\ref{tab:Routh}).

\begin{table}[htbp]
\centering
\begin{tabular}{c|c c c c c}
$s^n$     & $a_0$ & $a_2$ & $a_4$ & $a_6$ & ...\\
$s^{n-1}$ & $a_1$ & $a_3$ & $a_5$ & $a_7$ & ...\\
$s^{n-2}$ & $b_1$ & $b_3$ & $b_5$ & $b_7$ & ...\\
$s^{n-3}$ & $c_1$ & $c_3$ & $c_5$ & $c_7$ & ...\\
$s^{n-4}$ & $d_1$ & $d_3$ & $d_5$ & $d_7$ & ...\\
\vdots                                         \\
$s^2$ & $e_1$ & $e_2$                          \\
$s^1$ & $f_1$                                  \\
$s^0$ & $g_1$
\end{tabular}
\caption{\label{tab:Routh}Ejemplo de tabla de Routh.}
\end{table}

Donde:

\begin{math}
b_1 = \frac{a_1 a_2 - a_0 a_3}{a_1} , b_2 = \frac{a_1 a_4 - a_0 a_5}{a_1} , \dots
\end{math}

\begin{math}
c_1 = \frac{b_1 a_3 - a_1 b_2}{b_1} , c_2 = \frac{b_1 a_5 - a_1 b_3}{b_1} , \dots
\end{math}

\begin{math}
d_1 = \frac{c_1 b_2 - b_1 c_2}{c_1} , d_2 = \frac{c_1 b_3 - b_1 c_3}{c_1} , \dots
\end{math}

\begin{math}
\vdots
\end{math}
\item
El número de raíces con parte real positiva es igual al numero de cambios de signo en la primera columna(Ver Cuadro~\ref{tab:Numeros}).

\begin{table}[htbp]
\centering
\begin{tabular}{c|c|}
$s^n$     & $a_0$ \\
$s^{n-1}$ & $a_1$ \\
$s^{n-2}$ & $b_1$ \\
$s^{n-3}$ & $c_1$ \\
$s^{n-4}$ & $d_1$ \\
\vdots & \vdots   \\
$s^2$ & $e_1$     \\
$s^1$ & $f_1$     \\
$s^0$ & $g_1$
\end{tabular}
\caption{\label{tab:Numeros}Números en los que hay que revisar el cambio de signo.}
\end{table}

\end{enumerate}

\subsection{Casos Especiales}
\begin{enumerate}

\item
En los casos en los que un coeficiente es $0$ se puede intercambiar por un $\epsilon$ lo suficientemente pequeño para aproximar a $0$ (Véase el Cuadro~\ref{tab:Caso1}).

\begin{math}
A(s) = s^3 + 2 s^2 + s + 2 = 0
\end{math}

\begin{verbatim}
>> A = [1 2 1 2];
>> r = roots(A)
r =
  -2.00000 + 0.00000i
  -0.00000 + 1.00000i
  -0.00000 - 1.00000i
\end{verbatim}

\begin{table}[htbp]
\centering
\begin{tabular}{c|c c}
$s^3$ & $1$ & $1$ \\
$s^2$ & $2$ & $2$ \\
$s^1$ & $0 \approx \epsilon$ \\
$s^0$ & $2$
\end{tabular}
\caption{\label{tab:Caso1}Caso Especial 1.}
\end{table}

\item
Cuando existen cambios en los coeficientes del polinomio característico se sabe que existirán raíces con parte real positiva (Véase el cuadro~\ref{tab:Caso2}).

\begin{math}
A(s) = s^3 - 3 s + 2 = 0
\end{math}

\begin{verbatim}
>> A = [1 0 -3 2];
>> r = roots(A)
r =
  -2.00000
   1.00000
   1.00000
\end{verbatim}

\begin{table}[htbp]
\centering
\begin{tabular}{c|c c}
$s^3$ & $1$ & $-3$ \\
$s^2$ & $0\approx\epsilon$ & $2$ \\
$s^1$ & $-\frac{2}{\epsilon}$ & $0$ \\
$s^0$ & $2$
\end{tabular}
\caption{\label{tab:Caso2}Caso Especial 2.}
\end{table}

\item
Cuando todos los coeficientes en una linea se eliminan se puede crear un nuevo polinomio auxiliar con la linea anterior, obtener su derivada e insertar en la siguiente linea para continuar calculando la tabla (Véase el Cuadro~\ref{tab:Caso3a} y~\ref{tab:Caso3b}).

\begin{math}
A(s) = s^5 + 2 s^4 + 24 s^3 + 48 s^2 - 25 s - 50 = 0
\end{math}

\begin{math}
p_{aux}(s) = 2 s^4 + 48 s^2 - 50
\end{math}

\begin{math}
\frac{\mathrm d}{\mathrm d x} p_{aux}(s) = 8 s^3 + 96 s
\end{math}

\begin{verbatim}
>> A = [1 2 24 48 -25 -50];
>> r = roots(A)
r =
  -0.00000 + 5.00000i
  -0.00000 - 5.00000i
   1.00000 + 0.00000i
  -2.00000 + 0.00000i
  -1.00000 + 0.00000i
\end{verbatim}

\begin{table}[htbp]
\centering
\begin{tabular}{c|c c c}
$s^5$ & $1$ & $24$ & $-25$ \\
$s^4$ & $2$ & $48$ & $-50$ \\
$s^3$ & $0$ & $0$  & $0$   \\
$s^2$ \\
$s^1$ \\
$s^0$
\end{tabular}
\caption{\label{tab:Caso3a}Caso Especial 3a.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{c|c c c}
$s^5$ & $1$ & $24$ & $-25$ \\
$s^4$ & $2$ & $48$ & $-50$ \\
$s^3$ & $8$ & $96$ & $0$   \\
$s^2$ & $24$ & $-50$ & $0$ \\
$s^1$ & $112.6$ & $0$ \\
$s^0$ & $-50$
\end{tabular}
\caption{\label{tab:Caso3b}Caso Especial 3b.}
\end{table}

\end{enumerate}
\subsection{Aplicación del criterio de Routh}
Si bien los sistemas numéricos actuales permiten el calculo de las raíces de un sistema de manera mas rápida y sencilla que con la aplicación de este método, aun existen aplicaciones practicas en las que es de suma importancia el determinar el numero de raíces positivas. Por ejemplo podemos tener ganancias en un sistema para las que queremos determinar de primera instancia, un rango de valores para los cuales el sistema no se volverá inestable.

Para ello calculamos la tabla de Routh de la misma manera en que lo hicimos anteriormente, pero teniendo en cuenta las ganancias a incluir en el calculo de las raíces (Por ejemplo con una ganancia proporcional véase Cuadro~\ref{tab:Aplicacion}).

\begin{table}[htbp]
\centering
\begin{tabular}{c|c c c c c}
$s^n$     & $a_0$ & $a_2$ & $a_4$ & $a_6$ & ...\\
$s^{n-1}$ & $a_1$ & $a_3$ & $a_5$ & $a_7$ & $k$\\
$s^{n-2}$ & $b_1$ & $b_3$ & $b_5$ & $b_7$ & ...\\
$s^{n-3}$ & $c_1$ & $c_3$ & $c_5$ & $c_7$ & ...\\
$s^{n-4}$ & $d_1$ & $d_3$ & $d_5$ & $d_7$ & ...\\
\vdots                                         \\
$s^2$ & $e_1$ & $e_2$                          \\
$s^1$ & $f_1$                                  \\
$s^0$ & $g_1$
\end{tabular}
\caption{\label{tab:Aplicacion}Aplicación del criterio de Routh.}
\end{table}

\subsubsection{Ejemplo:}
Se toma el sistema $\frac{\hat{y}(s)}{\hat{R}(s)} = \frac{k}{s^4 + 3 s^3 + 3 s^2 + 2 s + k}$, entonces el polinomio característico del sistema será $F(s) = s^4 + 3 s^3 + 3 s^2 + 2 s + k$.

Construimos su tabla de Routh (Cuadro~\ref{tab:EjemploAplicacion}):

\begin{table}[htbp]
\centering
\begin{tabular}{c|c c c}
$s^4$ & $1$ & $3$ & $k$ \\
$s^3$ & $3$ & $2$ & $0$ \\
$s^2$ & $\frac{7}{3}$ & $k$ & $0$ \\
$s^1$ & $2 - \frac{9}{7} k$ & $0$ \\
$s^0$ & $k$
\end{tabular}
\caption{\label{tab:EjemploAplicacion}Ejemplo de Aplicación del criterio de Routh.}
\end{table}

De lo anterior podemos concluir que, para que no existan cambios de signos, toda la primera columna tiene que ser positiva, por lo que $k > 0$ y  $2 - ^9/_7 k > 0$, por lo que el rango de valores que puede ocupar la ganancia $k$ es $0 < k < ^{14}/_9$

Si bien esto no nos aporta una ganancia especifica para un comportamiento deseado, si nos da la pauta a los valores a tomar en cuenta, si no se desea que el sistema sea inestable.

\subsection{Acción Proporcional}
Tenemos un sistema de primer orden, al que le agregaremos un controlador de ganancia proporcional y una retroalimentación negativa, por lo que las ecuaciones que describen la salida y el error del sistema quedan:

\begin{equation}
\frac{\hat{y}(s)}{\hat{R}(s)} = \frac{k}{Ts + 1 + k}
\end{equation}

\begin{equation}
\frac{\hat{e}(s)}{\hat{R}(s)} = \frac{R(s) - Y(s)}{R(s)} = \frac{Ts + 1}{Ts + 1 + k}
\end{equation}

\subsubsection{Estabilidad}
El problema reside en encontrar un conjunto de ganancias $k$ para las cuales el sistema es estable.
\begin{equation}
F(s) = s + \frac{1 + k}{T}
\end{equation}

Aplicamos una tabla de Routh a este polinomio característico (Cuadro~\ref{tab:AccionProporcional}).

\begin{table}[htbp]
\centering
\begin{tabular}{c|c}
$s^1$ & $1$ \\
$s^0$ & $\frac{1+k}{T}$
\end{tabular}
\caption{\label{tab:AccionProporcional}Tabla de Routh para acción proporcional.}
\end{table}

Por lo que concluimos que la ganancia $k$ debe de seguir: $k>-1$

\subsubsection{Error en el estado permanente al escalón unitario}
También es importante investigar el error que causara el controlador al introducirse. Si ponemos como señal de referencia al escalón unitario($R(s) = \frac{1}{s}$), podemos ver lo siguiente:

\begin{math}
\displaystyle \lim_{t \to \infty} e(t) = \lim_{s \to 0} s e(s) = \lim_{s \to 0} \frac{Ts + 1}{Ts + 1 + k} = \frac{1}{1 + k}
\end{math}

\subsection{Acción Integral}
Tenemos un sistema de primer orden, al que le agregaremos un controlador de ganancia integral y una retroalimentación negativa, por lo que las ecuaciones que describen la salida y el error del sistema quedan:

\begin{equation}
\frac{\hat{y}(s)}{\hat{R}(s)} = \frac{k}{s(Ts + 1) + k}
\end{equation}

\begin{equation}
\frac{\hat{e}(s)}{\hat{R}(s)} = \frac{R(s) - Y(s)}{R(s)} = \frac{s(Ts + 1)}{s(Ts + 1) + k}
\end{equation}

\subsubsection{Estabilidad}
El problema reside en encontrar un conjunto de ganancias $k$ para las cuales el sistema es estable.
\begin{equation}
F(s) = s^2 + \frac{1}{T} s + \frac{k}{T}
\end{equation}

Aplicamos una tabla de Routh a este polinomio característico (Cuadro~\ref{tab:AccionIntegral}).

\begin{table}[htbp]
\centering
\begin{tabular}{c|c c}
$s^2$ & $1$ & $\frac{k}{T}$ \\
$s^1$ & $\frac{1}{T}$ & $0$ \\
$s^0$ & $\frac{k}{T}$
\end{tabular}
\caption{\label{tab:AccionIntegral}Tabla de Routh para acción integral.}
\end{table}

Por lo que concluimos que la ganancia $k$ debe de seguir: $k>0$

\subsubsection{Error en el estado permanente al escalón unitario}
También es importante investigar el error que causara el controlador al introducirse. Si ponemos como señal de referencia al escalón unitario($R(s) = \frac{1}{s}$), podemos ver lo siguiente:

\begin{math}
\displaystyle \lim_{t \to \infty} e(t) = \lim_{s \to 0} s e(s) = \lim_{s \to 0} s \left(\frac{s(Ts + 1)}{s(Ts + 1) + k} \frac{1}{s}\right) = 0
\end{math}

\subsection{Acción Proporcional Integral}
\subsubsection{Estabilidad}
\subsubsection{Error en el estado permanente al escalón unitario}

\newpage
\section{Lugar de las Raíces}
Si tenemos un sistema con retroalimentación, su polinomio característico es el siguiente:
\begin{equation}
F(s) = 1 + H(s) G(s) = 0
\end{equation}

Donde $G(s)$ es la planta y $H(s)$ es el elemento de retroalimentación. Las condiciones de angulo y magnitud son las siguientes:

\begin{equation}
\angle H(s) G(s) = \pm 180^{\circ} (2R + 1) \mid R \in \mathbbm{Z}^+
\end{equation}

\begin{equation}
\lvert H(s) G(s) \rvert = 1
\end{equation}

De aquí notamos que la condición de angulo, nos da la forma del lugar de las raíces, y la condición de magnitud nos da su posición.

Pues bien, para trazar el lugar geométrico de las raíces seguimos una serie de pasos enumerados a continuación:

\begin{enumerate}
\item
Determinar el lugar de las raíces en el eje real.

Ejemplo: $H(s) = 1$, $G(s) = \frac{k}{s(s+1)(s+2)}$


\item
Determinar las asintotas del lugar de las raíces.
\item
Determinar el punto de ruptura o partida de las asintotas en el eje real.
\item
Determinar los puntos donde el lugar de las raíces atraviesa el eje imaginario.
\end{enumerate}

\newpage
\section{Compensador Adelanto/Atraso (LR)}
\subsection{Compensador de adelanto de fase}
\subsection{Compensador de atraso de fase}
\subsubsection{Error estático de posición $k_p$}
\subsubsection{Error estático de velocidad $k_v$}

\newpage
\section{Diagramas de Bode}
\subsection{Factor integral}
\subsection{Factor derivativo}
\subsection{Factores de primer orden}
\subsection{Factores de segundo orden}
\subsection{Frecuencia de resonancia $\omega_n$ y valor par de resonancia $M_R$}

\newpage
\section{Diagramas de Nyquist}
\subsection{Factor integral}
\subsection{Factor derivativo}
\subsection{Factores de primer orden}
\subsection{Factores de segundo orden}

\newpage
\section{Criterio de Estabilidad de Nyquist}
\subsection{Ejemplos}

\newpage
\section{Estabilidad Relativa}
\subsection{Margen de Fase}
\subsubsection{Estable}
\subsubsection{Inestable}
\subsection{Margen de Ganancia}
\subsubsection{Estable}
\subsubsection{Inestable}

\newpage
\section{Compensador de adelanto y atrase de fase (Frecuencia)}
\subsection{Compensador de adelanto de fase}
\subsection{Compensador de atraso de fase}
\subsection{Ejemplos}

\newpage
\section{Controladores PID}
\subsection{Sintonización: Reglas de Ziegler-Nichols}
\subsubsection{Respuesta al escalón}
\subsubsection{Respuesta a oscilaciones sostenidas}
\subsection{Esquemas modificados}
\subsubsection{Controlador PID}
\subsubsection{Controlador PI-D}
\subsubsection{Controlador I-PD}

\newpage
\section{Representación de estado}

La siguiente funcion de transferencia es la Transformada de Laplace de la ecuacion diferencial ordinaria de orden $n$ que describe al sistema.

\begin{equation}
\frac{\hat{y}(s)}{\hat{u}(s)} = \frac{b_0 s^m + b_1 s^{m-1} + ... + b_{m-1} s + b_m}{s^n + a_1 s^{n-1} + ... + a_{n-1} s + a_n} = \frac{B(s)}{A(s)}, m \le n
\end{equation}

\begin{multline}
\frac{d^n}{dt^n} y(t) + a_1 \frac{d^{n-1}}{dt^{n-1}} y(t) + \dots + a_{n-1} \frac{d}{dt} y(t) + a_n \frac{d}{dt} y(t) \\ = b_0 \frac{d^m}{dt^m} u(t) + b_1 \frac{d^{m-1}{dt^{m-1}}} u(t) + \dots + b_{m-1} \frac{d}{dt} u(t) + b_{m} u(t)
\end{multline}

Haciendo la siguiente asignacion de variables:

\begin{eqnarray}
x_1     & = & z \nonumber \\
x_2     & = & \frac{d}{dt} x_1 = \frac{d}{dt} z \nonumber \\
x_3     & = & \frac{d}{dt} x_2 = \frac{d^2}{dt^2} z \nonumber \\
\vdots  & = & \vdots \nonumber \\
x_{n-1} & = & \frac{d}{dt} x_{n-2} = \frac{d^{n-2}}{dt^{n-2}} z \nonumber \\
x_n     & = & \frac{d}{dt} x_{n-1} = \frac{d^{n-1}}{d^{n-1}} z \nonumber
\end{eqnarray}

Donde:

\begin{equation}
\frac{d}{dt} x_n = -a_n x_1 - a_{n-1} x_2 - \dots - a_2 x_{n-1} - a_1 x_n + u(t)
\end{equation}
\begin{equation}
y = b_m x_1 + b_{m-1} x_2 + \dots + b_1 x_{m-1} + b_0 x_m
\end{equation}

Por lo que se obtiene:

\begin{math}
\left( \frac{d^n}{dt^n} + a_1 \frac{d^{n-1}}{dt^{n-1}} + \dots + a_{n-1} \frac{d}{dt} + a_n \right) z(t) = u(t)
\end{math}

\begin{math}
y(t) = \left( b_m + b_{m-1} \frac{d}{dt} + \dots + b_1 \frac{d^{m-1}}{dt^{m-1}} + b_0 \frac{d^m}{dt^m} \right) z(t)
\end{math}

es decir:

\begin{equation}
M \left( \frac{d}{dt} \right) z(t) = u(t)
\end{equation}

\begin{equation}
y(t) = N \left( \frac{d}{dt} \right) z(t)
\end{equation}

Lo cual implica $M \left( \frac{d}{dt} \right) y(t) = N \left( \frac{d}{dt} \right) u(t)$. Donde:

\begin{math}
M \left( \frac{d}{dt} \right) = \left( \frac{d^n}{dt^n} + a_1 \frac{d^{n-1}}{dt^{n-1}} + \dots + a_{n-1} \frac{d}{dt} + a_n \right)
\end{math}

\begin{math}
N \left( \frac{d}{dt} \right) = \left( b_m + b_{m-1} \frac{d}{dt} + \dots + b_1 \frac{d^{m-1}}{dt^{m-1}} + b_0 \frac{d^m}{dt^m} \right)
\end{math}

Esta es la misma ecuación diferencial con la que empezamos. Note que la escritura matricial de esta Ecuación Diferencial Ordinaria es:

\begin{equation}
\frac{d}{dt} \vec{x} = A \vec{x}(t) + \vec{b} u(t)
\end{equation}

\begin{equation}
\vec{y}(t) = \vec{c} \cdot \vec{x}(t)
\end{equation}

Donde:

\begin{equation}
\vec{x}(t) =
\begin{pmatrix}
x_1(t) \\
x_2(t) \\
\vdots \\
x_n(t) 
\end{pmatrix}
\end{equation}

\begin{equation}
A =
\begin{pmatrix}
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
0 & 0 & 0 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots &   & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 1 \\
-a_n & -a_{n-1} & -a_{n-2} & \dots & -a_2 & -a_1 
\end{pmatrix}
\end{equation}

\begin{equation}
\vec{b} =
\begin{pmatrix}
0 \\
0 \\
\vdots \\
1
\end{pmatrix}
\end{equation}

\begin{equation}
\vec{c} =
\begin{pmatrix}
b_m     \\
b_{m-1} \\
\vdots  \\
b_1     \\
b_0     \\
\vdots  \\
0       \\
0
\end{pmatrix}
\end{equation}

\subsection{Solución temporal de la ecuación de estado}

\begin{enumerate}

\item
Para el caso en que $A$ es un escalar y la solución es homogénea se considera la siguiente Ecuación Diferencial Ordinaria:

\begin{equation}
\frac{d}{dt} x(t) = a x(t) \mid x(0) = x_0
\end{equation}

Suponga una solución de la forma:

\begin{equation}
x(t) = \alpha_0 + \alpha_1 t + \alpha_2 t^2 + \dots + \alpha_k t^k + \dots
\end{equation}

Entonces se tiene:

\begin{multline}
\alpha_1 + 2 \alpha_2 t + 3 \alpha_3 t^2 + \dots + k \alpha_k t^{k-1} + \dots \\ = a \alpha_0 + a \alpha_1 t + a \alpha_2 t^2 + \dots + a \alpha_k t^k + \dots \nonumber
\end{multline}

Por lo que las $\alpha_i$ deben satisfacer:

\begin{equation}
\begin{array}{c c c c c}
\alpha_1 & = & a \alpha_0     & = & \frac{1}{1!} a^1 \alpha_0 \\
\alpha_2 & = & a \alpha_1     & = & \frac{1}{2!} a^2 \alpha_0 \\
\alpha_3 & = & a \alpha_2     & = & \frac{1}{3!} a^3 \alpha_0 \\
\vdots   & = & \vdots         & = & \vdots                    \\
\alpha_k & = & a \alpha_{k-1} & = & \frac{1}{k!} a^k \alpha_0 \\
\end{array} \quad ; \quad \alpha_0 = x_0
\end{equation}

Esto es:

\begin{equation}
x(t) = \left( \sum\limits_{i = 0}^{\infty} \frac{1}{i!} (a t)^i \right) x_0 \nonumber
\end{equation}

\begin{equation}
x(t) = e^{at} x_0
\end{equation}

Notese que: $\frac{d}{dt} x(t) = \left( \sum\limits_{i=0}^{\infty} \frac{1}{i!} \frac{d}{dt} (at)^i \right) x_0 = \left( \sum\limits_{i=1}^{\infty} \frac{1}{(i-1)!} (at)^{i-1} \right) a x_0 = \left( \sum\limits_{j=0}^{\infty} \frac{1}{j!} (at)^j \right) a x_0$

\begin{math}
\frac{d}{dt} x(t) = a e^{at} x_0 = a x(t) \quad x(0) = x_0
\end{math}

\item
Para el caso en que $A$ es una matriz y la solución es homogénea se considera la siguiente Ecuación Diferencial Ordinaria:

\begin{equation}
\frac{d}{dt} \vec{x}(t) = a \vec{x}(t) \mid \vec{x}(0) = \vec{x}_0
\end{equation}

De la misma manera que en el caso escalar, se supone una solución de la forma:

\begin{equation}
\vec{x}(t) = \vec{\alpha}_0 + \vec{\alpha}_1 t + \vec{\alpha}_2 t^2 + \dots + \vec{\alpha}_k t^k + \dots
\end{equation}

Entonces se tiene:

\begin{multline}
\vec{\alpha}_1 + 2 \vec{\alpha}_2 t + 3 \vec{\alpha}_3 t^2 + \dots + k \vec{\alpha}_k t^{k-1} + \dots \\ = A \vec{\alpha}_0 + A \vec{\alpha}_1 t + A \vec{\alpha}_2 t^2 + \dots + A \vec{\alpha}_k t^k + \dots \nonumber
\end{multline}

Por lo que las $\vec{\alpha}_i$ deben satisfacer:

\begin{equation}
\begin{array}{c c c c c}
\vec{\alpha}_1 & = & A \vec{\alpha}_0     & = & \frac{1}{1!} A^1 \vec{\alpha}_0 \\
\vec{\alpha}_2 & = & A \vec{\alpha}_1     & = & \frac{1}{2!} A^2 \vec{\alpha}_0 \\
\vec{\alpha}_3 & = & A \vec{\alpha}_2     & = & \frac{1}{3!} A^3 \vec{\alpha}_0 \\
\vdots   & = & \vdots         & = & \vdots                    \\
\vec{\alpha}_k & = & A \vec{\alpha}_{k-1} & = & \frac{1}{k!} A^k \vec{\alpha}_0 \\
\end{array} \quad ; \quad \vec{\alpha}_0 = \vec{x}_0
\end{equation}

Esto es:

\begin{equation}
\vec{x}(t) = \left( \sum\limits_{i = 0}^{\infty} \frac{1}{i!} (A t)^i \right) \vec{x}_0 \nonumber
\end{equation}

En análisis real, se demuestra que esta serie es absolutamente convergente y se define como:

\begin{equation}
\exp{(At)} = \sum\limits_{i = 0}^{\infty} \frac{1}{i!} (A t)^i
\end{equation}

Notese que:

\begin{math}
\frac{d}{dt} \exp{(At)} = \frac{d}{dt} \sum\limits_{i=0}^{\infty} \frac{1}{i!} (At)^i = \left( \sum\limits_{i=1}^{\infty} \frac{1}{(i-1)!} (At)^{i-1} \right) A = A \sum\limits_{j=0}^{\infty} \frac{1}{j!} (At)^j = A \exp{(At)}
\end{math}

Por lo que:

\begin{math}
\vec{x}(t) = \exp{(At)} \vec{x}_0
\end{math}

\begin{math}
\frac{d}{dt} \vec{x}(t) = A \exp{(At)} \vec{x}_0 = A \vec{x}(t) \quad \vec{x}(0) = \vec{x}_0
\end{math}

\item
Para el caso en que $A$ es escalar y la solución es forzada:

\begin{equation}
\frac{d}{dt} x(t) = a x(t) + b u(t) \mid x(0) = 0
\end{equation}

La solución a esta ecuación es:

\begin{equation}
x(t) = \int_0^t e^{a(t-\tau)} b u(\tau) \, d \tau
\end{equation}

\begin{math}
\frac{d}{dt} x(t) = e^{a(t-t)} b u(t) + \int_0^t \frac{d}{dt} e^{a(t-\tau)} b u(\tau) \, d \tau = b u(t) + a \int_0^t e^{a(t-\tau)} b u(\tau) \, d \tau
\end{math}

\begin{equation}
\frac{d}{dt} x(t) = b u(t) + a x(t)
\end{equation}

Por lo que la solución general (con $x(0) = x_0$):

\begin{equation}
x(t) = e^{at} x_0 + \int_0^t e^{a(t-\tau)} b u(\tau) \, d \tau
\end{equation}

\item
Para el caso en que $A$ es una matriz y la solución es forzada:

\begin{equation}
\frac{d}{dt} \vec{x}(t) = A \vec{x}(t) + \vec{b} u(t) \mid \vec{x}(0) = 0
\end{equation}

La solución de esta ecuación es:

\begin{equation}
\vec{x}(t) = \int_0^t \exp{(A(t-\tau))} \vec{b} u(\tau) \, d \tau
\end{equation}

En efecto, derivando tenemos:

\begin{math}
\frac{d}{dt} \vec{x}(t) = \exp{(A(t-t))} \vec{b} u(t) + \int_0^t \frac{d}{dt} \exp{(A(t-\tau))} \vec{b} u(\tau) \, d \tau = \vec{b} u(t) + A \int_0^t \exp{(A(t-\tau))} \vec{b} u(\tau) \, d \tau
\end{math}

\begin{equation}
\frac{d}{dt} \vec{x}(t) = \vec{b} u(t) + a \vec{x}(t)
\end{equation}

Por lo que la solución general (con $x(0) = x_0$):

\begin{equation}
\vec{x}(t) = \exp{(At)} \vec{x}_0 + \int_0^t \exp{(A(t-\tau))} \vec{b} u(\tau) \, d \tau
\end{equation}

\end{enumerate}

\subsection{Función (Matriz) de transferencia de la ecuación de estado}

\begin{enumerate}

\item
Para el caso escalar, se tiene que la transformada de Laplace con coeficientes independientes nulos es:

\begin{align}
s x(s)       & = a x(s) + b u(s) \nonumber\\
(s - a) x(s) & = b u(s) \nonumber\\
x(s)         & = (s - a)^{-1} b u(s) \nonumber\\
x(s)         & = \frac{b}{s - a} u(s) \nonumber
\end{align}

Por lo que:

\begin{equation}
e^{at} = \mathcal{L}^{-1} \left\{ (s - a)^{-1} \right\}
\end{equation}

\item
Para el caso matricial, tenemos que la transformada de Laplace con coeficientes independientes nulos es:

\begin{align}
s \vec{x}(s)         & = A \vec{x}(s) + \vec{b} u(s) \nonumber \\
(s I - A) \vec{x}(s) & = \vec{b} u(s) \nonumber \\
x(s)                 & = (s I - A)^{-1} \vec{b} u(s) \nonumber
\end{align}

Por lo que:

\begin{equation}
\exp{(At)} = \mathcal{L}^{-1} \left\{ (s I - A)^{-1} \right\}
\end{equation}

\end{enumerate}

\subsection{Función de transferencia de la representación de estado}

Sea la siguiente Ecuación Diferencial Ordinaria:

\begin{equation}
M \left( \frac{d}{dt} \right) y(t) = N \left( \frac{d}{dt} \right) u(t) \nonumber
\end{equation}

donde:

\begin{math}
M \left( \frac{d}{dt} \right) = \frac{d^n}{dt^n} + a_1 \frac{d^{n-1}}{dt^{n-1}} + \dots + a_{n-1} \frac{d}{dt} + a_n
\end{math}

\begin{math}
N \left( \frac{d}{dt} \right) = b_m + b_{m-1} \frac{d}{dt} + \dots + b_1 \frac{d^{m-1}}{dt^{m-1}} + b_0 \frac{d^m}{dt^m}
\end{math}

La función de transferencia con coeficientes independientes nulos de las ecuaciones es:

\begin{equation}
F(s) = \frac{N(s)}{M(s)} = \frac{\frac{d^n}{dt^n} + a_1 \frac{d^{n-1}}{dt^{n-1}} + \dots + a_{n-1} \frac{d}{dt} + a_n}{b_m + b_{m-1} \frac{d}{dt} + \dots + b_1 \frac{d^{m-1}}{dt^{m-1}} + b_0 \frac{d^m}{dt^m}}
\end{equation}

\begin{description}
\item [Ceros.] Las raíces del polinomio $N(s)$.
\item [Polos.] Las raíces del polinomio $M(s)$.
\end{description}

Sea la siguiente representación de estado de la Ecuación Diferencial Ordinaria:

\begin{eqnarray}
\frac{d}{dt} x & = & A x + b u \nonumber \\
y & = & c x + d u \nonumber
\end{eqnarray}

La función de transferencia con coeficientes independientes nulos en esta representación es:

\begin{eqnarray}
s x(s) & = & A x(s) + b u(s) \nonumber \\
y(s) & = & c x(s) + d u(s) \nonumber
\end{eqnarray}

\begin{equation}
\vdots \nonumber
\end{equation}

\begin{eqnarray}
(s I - A) x(s) & = & b u(s) \nonumber \\
y(s) & = & c x(s) + d u(s) \nonumber
\end{eqnarray}

\begin{equation}
\vdots \nonumber
\end{equation}

\begin{eqnarray}
x(s) & = & (s I - A)^{-1} b u(s) \nonumber \\
y(s) & = & c x(s) + d u(s) \nonumber
\end{eqnarray}

\begin{equation}
\vdots \nonumber
\end{equation}

\begin{equation}
y(s) = c[(s I - A)^{-1} b u(s)] + d u(s) = [c(s I - A)^{-1} b + d] u(s) \nonumber
\end{equation}

\begin{equation}
\vdots \nonumber
\end{equation}

\begin{equation}
F(s) = c(s I - A)^{-1} b + d
\end{equation}

\subsection{Matriz sistema}

\begin{equation}
\Sigma(s) =
\begin{pmatrix}
sI - A & b \\
-c & d
\end{pmatrix}
\end{equation}

Note que:

\begin{multline}
\begin{pmatrix}
(sI - A)^{-1} & 0 \\
0 & I
\end{pmatrix}
\begin{pmatrix}
sI - A & b \\
-c & d
\end{pmatrix}
\begin{pmatrix}
I & -(sI - A)b \\
0 & I
\end{pmatrix}
\\
=
\begin{pmatrix}
I & (sI - A)^{-1} b \\
-c & d
\end{pmatrix}
\begin{pmatrix}
I & -(sI - A)b \\
0 & I
\end{pmatrix}
\\
=
\begin{pmatrix}
I & 0 \\
-c & (c(sI - A)^{-1} b + d)
\end{pmatrix}
\nonumber
\end{multline}

Por lo que:

\begin{math}
\det{\left( (sI - A)^{-1} \right)} \cdot \det{\left( \Sigma(s) \right)} \cdot I = c(sI - A)^{-1} b + d
\end{math}

\begin{equation}
F(s) = \frac{\det{\left( \Sigma(s) \right)}}{\det{\left( sI - A \right)}}
\end{equation}

Por lo que los polos coinciden con los valores propios de $A$ y los ceros son los números complejos que hacen perder rango a la matriz sistema.

\begin{equation}
\text{Polos: } F(s) = \left\{ s \in \mathbbm{C} \mid \det{\left( sI - A \right)} = 0 \right\}
\end{equation}

\begin{equation}
\text{Ceros: } F(s) = \left\{ s \in \mathbbm{C} \mid \det{\left( \Sigma(s) \right)} = 0 \right\}
\end{equation}

\subsection{Propiedades de la Matriz A}
\begin{enumerate}[i)]
\item 
\begin{equation}
\exp{(A t)} = \sum\limits_{i=0}^{\infty} \frac{1}{i!} (A t)^i
\end{equation}

\item
\begin{equation}
\frac{d}{dt} \exp{(A t)} = A \exp{(A t)} = (\exp{(A t)}) A
\end{equation}

\item

\begin{multline}
\exp{(At)} \exp{(A \tau)} = \left( \sum\limits_{i=0}^{\infty} \frac{1}{i!} (A t)^i \right) \left( \sum\limits_{j=0}^{\infty} \frac{1}{j!} (A \tau)^j \right) \\
= \sum\limits_{i=0}^{\infty} \sum\limits_{j=0}^{\infty} A^{i+j} \frac{t^i \tau^j}{i! j!} = \sum\limits_{k=0}^{\infty} A^k \sum\limits_{i=0}^{k} \frac{t^i \tau^{k-i}}{i! (k-i)!} \\
= \sum\limits_{k=0}^{\infty} A^k \frac{(t + \tau)^k}{k!} = \exp{(A(t + \tau))}
\end{multline}

\item
\begin{equation}
\exp{((A + B) t)} = \exp{(A t)} \exp{(B t)} \iff A B = B A
\end{equation}

\item Cambio de base.

Sean dos matrices similares $A$ y $\bar{A}$, esto es, dos matrices relacionadas por un cambio de base, $T$ matriz invertible, esto es $\bar{A} = T^{-1} A T$.

\begin{enumerate}[a)]
\item Las matrices exponenciales asociadas a las matrices $A$ y $\bar{A}$ también son similares. En efecto:

\begin{multline}
T^{-1} \exp{(At)} T = T^{-1} \left( \sum\limits_{i=0}^{\infty} \frac{1}{i!} (A t)^i \right) T = \sum\limits_{i=0}^{\infty} \frac{1}{i!} T^{-1} A^i T t^i \\
= \sum\limits_{i=0}^{\infty} \frac{1}{i!} (T^{-1} A T)^i t^i = \exp{\bar{A} t} \nonumber 
\end{multline}

\item Los valores propios son invariantes bajo cambio de base. En efecto:

\begin{multline}
\det{(sI - \bar{A})} = \det{(sI - T^{-1} A T)} = \det{(s T^{-1} T - T^{-1} A T)} \\
= \det{(T^{-1} (sI - A) T)} = \det{(T^{-1})} \det{(sI - A)} \det{(T)} \\
= \frac{1}{\det{(T)}} \det{(sI - A)} \det{(T)} = \det{(sI - A)} \nonumber
\end{multline}

\item Las raíces de la matriz sistema son invariantes bajo cambio de base. En efecto, sea el sistema representado por:

\begin{eqnarray}
\frac{d}{dt} x & = & A x + b u \nonumber \\
y & = & c x + d u \nonumber
\end{eqnarray}

Sea el cambio de variable $x = T \bar{x}$, $T$ invertible. Entonces:

\begin{eqnarray}
T \frac{d}{dt} \bar{x} & = & A T \bar{x} + b u \nonumber \\
y & = & c T \bar{x} + d u \nonumber
\end{eqnarray}

\begin{equation}
\vdots \nonumber
\end{equation}

\begin{eqnarray}
\frac{d}{dt} \bar{x} & = & T^{-1} A T \bar{x} + T^{-1} b u \nonumber \\
y & = & c T \bar{x} + d u \nonumber
\end{eqnarray}

\begin{equation}
\vdots \nonumber
\end{equation}

\begin{eqnarray}
\frac{d}{dt} \bar{x} & = & \bar{A} \bar{x} + \bar{b} u \nonumber \\
y & = & \bar{c} \bar{x} + d u
\end{eqnarray}

donde $\bar{A} = T^{-1} A T$, $\bar{b} = T^{-1} b$, $\bar{c} = c T$. La matriz sistema se puede escribir de la siguiente manera:

\begin{equation}
\Sigma =
\begin{pmatrix}
sI - A & b \\
-c & d
\end{pmatrix}
\implies
\bar{\Sigma} =
\begin{pmatrix}
sI - \bar{A} & \bar{b} \\
-\bar{c} & d
\end{pmatrix}
\end{equation}

Notese que:

\begin{equation}
\bar{\Sigma} =
\begin{pmatrix}
sI - \bar{A} & \bar{b} \\
-\bar{c} & d
\end{pmatrix} =
\begin{pmatrix}
T^{-1} & 0 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
sI - A & b \\
-c & d
\end{pmatrix}
\begin{pmatrix}
T & 0 \\
0 & 1
\end{pmatrix} \nonumber
\end{equation}

Por lo que:

\begin{equation}
\det{\bar{\Sigma}} = \det{T^{-1}} \det{\Sigma} \det{T} = \det{\Sigma} \nonumber
\end{equation}

\end{enumerate}

\item Forma de Jordan

Dada una matriz $A$, existe una matriz de cambio de base $T$, tal que:

\begin{equation}
T^{-1} A T = J = D + N
\end{equation}

donde $D$ es una matriz diagonal (conteniendo los valores propios) y $N$ es una matriz nilpotente ($\exists \gamma \in \mathbbm{N} \mid N^{\gamma} = 0$) de la forma:

\begin{equation}
D =
\begin{pmatrix}
\lambda_1 & 0 & \dots & 0 \\
0 & \lambda_2 & \dots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \dots & \lambda_n
\end{pmatrix} \nonumber
\end{equation}

\begin{equation}
N =
\begin{pmatrix}
0 & 1 & 0 & \dots & 0 \\
0 & 0 & 1 & \dots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & \dots & 1 \\
0 & 0 & 0 & \dots & 0
\end{pmatrix} \nonumber
\end{equation}

Note que $D N = N D$, por lo que:

\begin{equation}
\exp{((D + N) t)} = \exp{(D t)} \exp{(N t)}
\end{equation}

donde:

\begin{equation}
\exp{(D t)} =
\begin{pmatrix}
e^{\lambda_1 t} & 0 & \dots & 0 \\
0 & e^{\lambda_2 t} & \dots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \dots & e^{\lambda_n t}
\end{pmatrix} \nonumber
\end{equation}

\begin{equation}
\exp{(N t)} = \sum\limits_{i=0}^{\infty} \frac{1}{i!} (N t)^i = \sum\limits_{i=0}^{\gamma - 1} \frac{1}{i!} (N t)^i \nonumber
\end{equation}

\item Teorema de Cayley-Hamilton

Toda transformación lineal $A$ satisface su polinomio característico.

\begin{equation}
\Pi(s) = \det{(sI - A)} = s^n + \Pi_1 s^{n-1} + \dots + \Pi_{n-1} s + \Pi_n
\end{equation}

\begin{equation}
\Pi(A) = A^n + \Pi_1 A^{n-1} + \dots + \Pi_{n-1} A + \Pi_n I = 0
\end{equation}

Una implicación directa es que la $n$-esima potencia de una transformación lineal $A$, es una combinación lineal de sus potencias predecesoras.

\begin{equation}
A^n = - \Pi_n I - \Pi_{n-1} A - \dots - \Pi_1 A^{n-1} \nonumber
\end{equation}

A su vez, esto implica:

\begin{equation}
\exp{(A t)} = \sum\limits_{i=0}^{\infty} \frac{1}{i!} A^i t^i = \sum\limits_{i=0}^{n-1} \varphi(t) A^n
\end{equation}

donde:

\begin{equation}
\varphi_i(t) = \sum\limits_{j=0}^{\infty} \varphi_{ij} t^j \nonumber
\end{equation}

\end{enumerate}

\newpage
\section{Controlabilidad y asignación de polos}

Sea un sistema para la Ecuación Diferencial Ordinaria:

\begin{equation}
M \left(\frac{d}{dt} \right) y(t) = N \left(\frac{d}{dt} \right) u(t)
\end{equation}

Sea la siguiente representación de estado de esta Ecuación Diferencial Ordinaria:

\begin{eqnarray}
\frac{d}{dt} x & = & A x + b u \nonumber \\
y & = & c x + d u \nonumber
\end{eqnarray}

donde $x \in \mathbbm{R}^n$ y $u,y \in \mathbbm{R}$.

Problema. Se desea encontrar una ley de control $u = f(x)$, que nos permita asignar los polos a voluntad.

Sabemos que:

\begin{description}
\item [Polos.] $\left\{ s \in \mathbbm{C} \mid M(s) = 0 \right\} = \left\{ s \in \mathbbm{C} \mid \det{(sI - A)} \right\}$
\end{description}

Para resolver este problema hay que investigar el concepto estructural de la alcanzabilidad.

\subsection{Alcanzabilidad y Controlabilidad}

\begin{itemize}
\item Una representación de estado se dice controlable, si para cualquier condición inicial, $x(0) = x_0 \in \mathbbm{R}^n$, existe una trayectoria, $x(\cdot)$, solución de la ecuación de estado, tal que en tiempo finito $t_f \in \mathbbm{R}$ se llega al origen $\left( x(t_f) = 0 \right)$.
\item Una representación de estado se dice alcanzable, si para cualquier punto $x \in \mathbbm{R}$, existe una trayectoria, $x(\cdot)$, solución de la ecuación de estado, tal que en tiempo finito $t_f \in \mathbbm{R}$ se llega a un punto cualquiera $\left( x(t_f) = x_f \right)$ desde el origen.
\end{itemize}

En los sistemas lineales estas dos propiedades están mutuamente implicadas, por lo que se les trata indistinguiblemente. Pero en general:

\begin{equation}
\text{Alcanzabilidad} \implies \text{Controlabilidad}
\end{equation}

La solución temporal de $\frac{dx}{dt} = Ax + bu$ con $x(0) = 0$ es:

\begin{equation}
x(t) = \int_0^t \exp{(A(t - \tau))} b u(\tau) d \tau
\end{equation}

del teorema de Cayley-Hamilton se tiene:

\begin{equation}
\exp{(A t)} = \sum\limits_{i=0}^{\infty} \frac{1}{i!} A^i t^i = \sum\limits_{i=0}^{n-1} \varphi_i(t) A^i
\end{equation}

donde $\varphi_i(t) = \sum\limits_{j=0}^{\infty} \varphi_{ij} t^j$, $\varphi_{ij} \in \mathbbm{R}$, $n \in \mathbbm{N}$, $j \in \mathbbm{Z}^+$. Por lo anterior, tenemos:

\begin{eqnarray}
x(t) & = & \sum\limits_{i=0}^{n-1} \psi_i(t) A^i b \nonumber \\
x(t) & = &
\begin{pmatrix}
b & A b & \dots & A^{n-1} b
\end{pmatrix}
\begin{pmatrix}
\psi_0(t) \\
\psi_1(t) \\
\vdots \\
\psi_{n-1}(t)
\end{pmatrix}
\end{eqnarray}

donde $\psi_i(t) = \int_0^t \varphi_i (t - \tau) u(\tau) d \tau$.

Entonces una condición necesaria para que $x(t_f) = x_f \quad \forall x \in \mathbbm{R}, \forall t_f \in \mathbbm{R}, t_f > 0$, es que la matriz de controlabilidad

\begin{equation}
C_{(A,b)} =
\begin{pmatrix}
b & Ab & \dots & A^{n-1}b
\end{pmatrix}
\end{equation}

sea de rango pleno por filas, de lo contrario existen componentes de $x(t)$ que siempre seran nulos. En nuestro caso particular $\left( y, u \in \mathbbm{R}^n \right)$:

\begin{equation}
\det{C_{(A,b)}} \ne 0
\end{equation}

Si la matriz de controlabilidad $C_{(A,b)}$ es de rango pleno por filas, entonces el gramiano de controlabilidad es invertible. \footnote{Aquí se esta abusando de la notación, ya que el gramiano de controlabilidad corresponde al caso en que $t \to \infty$}

\begin{equation}
W = \int_0^t \exp{(A \sigma)} b b^t \exp{(A^t \sigma)} d\sigma \quad t > 0, \sigma = t - \tau
\end{equation}

Entonces, con la siguiente ley de control se tiene:

\begin{equation}
u(t) = b^t \exp{(A^t(t_f - t))} W_{t_f}^{-1} x_f
\end{equation}

Por lo que si sustituimos $t_f$ en la solución para $x(t)$:

\begin{multline}
x(t_f) = \int_0^{t_f} \exp{(A(t - \tau))} b u(\tau) d\tau \\
       = \int_0^{t_f} \exp{(A(t - \tau))} b b^t \exp{(A^t(t_f - \tau))} W_{t_f}^{-1} x_f d\tau \\
       = \int_{t_f}^0 \exp{(A \sigma)} b b^t \exp{(A^t \sigma)} d\sigma W_{t_f}^{-1} x_f \\
       = W_{t_f} W_{t_f}^{-1} x_f = x_f
\end{multline}

\begin{itemize}
\item Por lo que una condición suficiente y necesaria para que la ecuación de estado sea alcanzable (y por lo tanto controlable), es que su matriz de controlabilidad, $C_{(A,b)}$, sea de rango pleno por filas.
\item Cuando la matriz de controlabilidad es de rango pleno por filas, se dice que el par $(A, b)$ es controlable.
\end{itemize}

\subsection{Asignación de polos}

Sea la ecuación de estado controlable, es decir $\frac{dx}{dt} = Ax + bu$ con $b \ne 0$, $\det{(C_{(A,b)})} \ne 0$, $\Pi(s)$ y $\alpha(s)$ los polinomios característico y mínimo de $A$ respectivamente.

\begin{equation}
\Pi(s) = \det{(sI - A)} \quad \text{grado } \Pi(s) = n \nonumber
\end{equation}

\begin{equation}
\alpha(s) \text{ es el polinomio de menor grado tal que } \alpha(A) = 0 \nonumber
\end{equation}

Sea $\kappa = \text{grado } \alpha(s)$, donde obviamente $1 \le \kappa \le n$.

\begin{equation}
\alpha(s) = s^{\kappa} + (a_{\kappa} + a_{\kappa - 1} s + \dots + a_1 s^{\kappa - 1}) \nonumber
\end{equation}

\begin{equation}
\alpha(A) = A^{\kappa} + (a_{\kappa} + a_{\kappa - 1} A + \dots + a_1 A^{\kappa - 1}) \nonumber
\end{equation}

Sean $\alpha_i$ con $i \in \{ 0, 1, \dots, \kappa \}$, los polinomios mónicos auxiliares tales que:

\begin{eqnarray}
\alpha_0(s) & = & \alpha(s) \nonumber \\
\alpha_1(s) & = & s^{\kappa - 1} + (a_{\kappa - 1} + a_{\kappa - 2} s + \dots + a_1 s^{\kappa - 2}) \nonumber \\
\vdots \nonumber \\
\alpha_{\kappa - 1}(s) & = & s + a_1 \nonumber \\
\alpha_{\kappa}(s) & = & 1 \nonumber
\end{eqnarray}

en donde, por definición, $\alpha_1(A) \ne 0$ y $\alpha_0(A) = 0$.

Sea $b \ne 0$, un vector en $\mathbbm{R}^n$ tal que su polinomio mínimo coincide con $\alpha(s)$.

\begin{eqnarray}
\alpha_i(A) b & \ne & 0 \quad i \in \{ 1, 2, \dots, \kappa \} \nonumber \\
\alpha_0(A) b & = & 0 \nonumber
\end{eqnarray}

\begin{equation}
\vdots \nonumber
\end{equation}

\begin{eqnarray}
(A^{\kappa - 1} + (a_{\kappa - 1} + a_{\kappa - 2} A + \dots + a_1 A^{\kappa - 2})) b & \ne & 0 \nonumber \\
\alpha_0(A) b & = & 0 \nonumber
\end{eqnarray}

\begin{equation}
\vdots \nonumber
\end{equation}

\begin{eqnarray}
\begin{pmatrix}
b & Ab & \dots & A^{\kappa - 1} b
\end{pmatrix}
\begin{pmatrix}
a_{\kappa - 1} \\
a_{\kappa - 2} \\
\vdots \\
a_{1}
\end{pmatrix} & \ne & 0 \nonumber \\
\alpha_0(A) b & = & 0 \nonumber
\end{eqnarray}

Suponga que el par $(A,b)$ es controlable, por lo tanto $\det{C_{(A,b)}} \ne 0$, entonces:

\begin{equation}
\begin{pmatrix}
b & A b & \dots & A^{n-1} b
\end{pmatrix} v \ne 0 \quad \forall v \ne 0 \quad \therefore \kappa = n
\end{equation}

Por lo que el polinomio mínimo y el polinomio característico coinciden cuando el par $(A,b)$ es controlable. Definimos la base:

\begin{eqnarray}
e_n & = & \alpha_n(A) b = b \nonumber \\
e_{n-1} & = & \alpha_{n-1}(A) b = (A + a_1I) b = A e_n + a_1 e_n \nonumber \\
e_{n-2} & = & \alpha_{n-2}(A) b = (A^2 + (a_2I + a_1A)) b = A(A+a_1I) b + a_2 b = A e_{n-1} + a_2 e_n \nonumber \\
\vdots & = & \vdots \nonumber \\
e_1 & = & \alpha_1(A) b = A e_2 + a_{n-1} e_n
\end{eqnarray}

Note que sustituyendo $A$, tenemos:

\begin{equation}
\alpha(A) b = (A^n + (a_nI + a_{n-1}A + \dots + a_1 A^{n-1})) b = A e_1 + a_n e_n = 0
\end{equation}

Por lo que:

\begin{eqnarray}
e_n & = & b \nonumber \\
A e_{n} & = & e_{n-1} + a_1 e_n \nonumber \\
A e_{n-1} & = & e_{n-2} + a_2 e_n \nonumber \\
\vdots & = & \vdots \nonumber \\
A e_2 & = & e_1 + a_{n-1} e_n \nonumber \\
A e_1 & = & - a_{n} e_n
\end{eqnarray}

Entonces, bajo la base definida, las transformaciones lineales tienen la siguiente forma:

\begin{equation}
A_c = \left[ A \right]_{\{ e_1, e_2, \dots, e_n \}} =
\begin{pmatrix}
0 & 1 & 0 & \dots & 0 & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 & 0 \\
0 & 0 & 0 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 0 & 1 \\
-a_{n} & -a_{n-1} & -a_{n-2} & \dots & -a_{3} & -a_{2} & -a_{1}
\end{pmatrix}
\end{equation}

\begin{equation}
b_c = \left[ b \right]_{\{ e_1, e_2, \dots, e_n \}} =
\begin{pmatrix}
0 \\
0 \\
0 \\
\vdots \\
0 \\
0 \\
1
\end{pmatrix}
\end{equation}

Por lo tanto:

\begin{equation}
\frac{d}{dt} x_c =
\begin{pmatrix}
0 & 1 & 0 & \dots & 0 & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 & 0 \\
0 & 0 & 0 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 0 & 1 \\
-a_{n} & -a_{n-1} & -a_{n-2} & \dots & -a_{3} & -a_{2} & -a_{1}
\end{pmatrix} x_c +
\begin{pmatrix}
0 \\
0 \\
0 \\
\vdots \\
0 \\
0 \\
1
\end{pmatrix} u
\end{equation}

\begin{description}
\item [Observaciones.] \mbox{}\\
\begin{enumerate}
\item Polinomio Característico

\begin{equation}
\det{(sI - A_c)} = s^n + a_1 s^{n-1} + \dots + a_{n-1} s + a_n = \Pi(s) \nonumber
\end{equation}

\item Retroalimentación de Estado

Sea $u = f_c x_c + v$, donde $f_c = (a_n - \bar{a}_n)(a_{n-1} - \bar{a}_{n-1})\dots(a_1 - \bar{a}_1)$. Entonces el sistema de lazo cerrado es:

\begin{equation}
\frac{d}{dt} x_c = A_{f_c} x_c + v
\end{equation}

donde $A_{f_c} = A_c + b_c f_c$, es decir:

\begin{equation}
A_{f_c} =
\begin{pmatrix}
0 & 1 & 0 & \dots & 0 & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 & 0 \\
0 & 0 & 0 & \dots & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 0 & 1 \\
-\bar{a}_{n} & -\bar{a}_{n-1} & -\bar{a}_{n-2} & \dots & -\bar{a}_{3} & -\bar{a}_{2} & -\bar{a}_{1}
\end{pmatrix}
\end{equation}

y su polinomio característico es $\det{(sI - A_{f_c})} = s^n + \bar{a}_1 s^{n-1} + \dots + \bar{a}_{n-1} s + \bar{a}_n$.

\end{enumerate}
\end{description}

\subsection{Propiedades de la matriz de controlabilidad}

\begin{enumerate}
\item Matriz de controlabilidad del par $(A_c, b_c)$.

\begin{equation}
C_{(A_c,b_c)} =
\begin{pmatrix}
0 & 0 & 0 & \dots & 1 \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 1 & \dots & * \\
0 & 1 & * & \dots & * \\
1 & * & * & \dots & *
\end{pmatrix}
\end{equation}

\begin{equation}
\det{C_{(A_c, b_c)}} = \pm 1
\end{equation}

\item Invarianza de la matriz de controlabilidad bajo cambio de base

Sea el par $(A, b)$ controlable, sea $T$ una matriz de cambio de base y sean $A_1 = T^{-1} A T$ y $b_1 = T^{-1} b$ las matrices de nuestra nueva base.

\begin{multline}
C_{(A_1, b_1)} =
\begin{pmatrix}
b_1 & A_1 b_1 & \dots & A^{n-1} b_1
\end{pmatrix} = \\
\begin{pmatrix}
T^{-1} b & T^{-1} A T T^{-1} b & \dots & (T^{-1} A T \dots T^{-1} A T) T^{-1} b
\end{pmatrix} = \\
\begin{pmatrix}
T^{-1} b & T^{-1} A b & \dots & T^{-1} A^{n-1} b
\end{pmatrix} = \\
T^{-1}
\begin{pmatrix}
b & A b & \dots & A^{n-1} b
\end{pmatrix} =
T^{-1} C_{(A, b)} \nonumber
\end{multline}

\begin{equation}
T = C_{(A, b)} C_{(A_1, b_1)}^{-1}
\end{equation}

\item Invarianza de la matriz de controlabilidad bajo retroalimentación de estado $u = f x + v$



\item
\end{enumerate}

\subsection{Formas canónicas}
\subsubsection{Forma canónica controlador}
\subsubsection{Forma canónica controlabilidad}

\newpage
\section{Inobservabilidad y observador de estado}
\subsection{Observabilidad e inobservabilidad}
\subsection{Dualidad}
\subsection{Propiedades de la matriz de observabilidad}
\subsection{Formas canónicas}
\subsubsection{Forma canónica observador}
\subsubsection{Forma canónica observabilidad}
\subsection{Observador de estado}

\newpage
\section{Principio de Separación}

\end{document}